{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a078071-3cb1-4278-b375-5c45efa100ca",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>基于pytorch + LSTM 的古诗生成</h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967d209d-b78d-4244-acf5-116462793004",
   "metadata": {},
   "source": [
    "### 作业介绍: \n",
    "本课程使用pytorch框架, 完成NLP任务:古诗生成,使用的模型为 LSTM, 并训练了词向量, 支持随机古诗和藏头诗生成, 并且生成的古诗具有多变性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ea3e70-08b6-4758-971b-4f2d2807b906",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecdbbb6-e8ef-4d71-8aec-b6b30e3bcd4b",
   "metadata": {},
   "source": [
    "### 导包:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d162c71c-6dca-4c4a-830a-4e3e73a9dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2a9016-fe98-44ed-988c-ae8a42e4f741",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f2a818-a853-4fe8-a35d-ffa346d09202",
   "metadata": {},
   "source": [
    "### 生成切分文件:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65f00d6c-e537-478c-88b7-b273a121ff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(file=\"poetry_7.txt\", train_num=6000):\n",
    "    all_data = open(file, \"r\", encoding=\"utf-8\").read()\n",
    "    with open(\"split_7.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        split_data_7 = \" \".join(all_data)\n",
    "        f.write(split_data_7)\n",
    "    return split_data_7[:train_num * 64]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a29d1e-73cf-4430-b4e1-78b516ffcd23",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201d6941-ef95-4dc2-a528-f8e3c2020df0",
   "metadata": {},
   "source": [
    "### 训练词向量:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc0814c1-bfcd-42fe-9a67-95e5bc1a1e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vec(split_file=\"split_7.txt\", org_file=\"poetry_7.txt\", train_num=6000):\n",
    "    param_file = \"word_vec.pkl\"\n",
    "    org_data = open(org_file, \"r\", encoding=\"utf-8\").read().split(\"\\n\")[:train_num]\n",
    "    if os.path.exists(split_file):\n",
    "        all_data_split = open(split_file, \"r\", encoding=\"utf-8\").read().split(\"\\n\")[:train_num]\n",
    "    else:\n",
    "        all_data_split = split_text().split(\"\\n\")[:train_num]\n",
    "\n",
    "    if os.path.exists(param_file):\n",
    "        return org_data, pickle.load(open(param_file, \"rb\"))\n",
    "\n",
    "    models = Word2Vec(all_data_split, vector_size=128, workers=7, min_count=1)\n",
    "    pickle.dump([models.syn1neg, models.wv.key_to_index, models.wv.index_to_key], open(param_file, \"wb\"))\n",
    "    return org_data, (models.syn1neg, models.wv.key_to_index, models.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eea4ec-a0e2-4857-aea9-5078e9aaeed2",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1bbc2b-57e1-4cef-a9e7-92eced1879e7",
   "metadata": {},
   "source": [
    "### 构建数据集:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91c761d3-8d79-4a9f-b208-4685e8593c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Poetry_Dataset(Dataset):\n",
    "    def __init__(self, w1, word_2_index, all_data):\n",
    "        self.w1 = w1\n",
    "        self.word_2_index = word_2_index\n",
    "        self.all_data = all_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        a_poetry = self.all_data[index]\n",
    "\n",
    "        a_poetry_index = [self.word_2_index[i] for i in a_poetry]\n",
    "        xs = a_poetry_index[:-1]\n",
    "        ys = a_poetry_index[1:]\n",
    "        xs_embedding = self.w1[xs]\n",
    "\n",
    "        return xs_embedding, np.array(ys).astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b53b25d-7c27-46e9-9ed5-eeece1868313",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4873af03-36bb-44a5-9f24-7a16cb1be51c",
   "metadata": {},
   "source": [
    "### 模型构建:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d32e517d-646b-431d-9eb3-a74edc06bd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Poetry_Model_lstm(nn.Module):\n",
    "    def __init__(self, hidden_num, word_size, embedding_num):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        ######定义模型######\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.hidden_num = hidden_num\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=embedding_num, hidden_size=hidden_num, batch_first=True, num_layers=2,\n",
    "                            bidirectional=False)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.flatten = nn.Flatten(0, 1)\n",
    "        self.linear = nn.Linear(hidden_num, word_size)\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    def forward(self, xs_embedding, h_0=None, c_0=None):\n",
    "        if h_0 == None or c_0 == None:\n",
    "            h_0 = torch.tensor(np.zeros((2, xs_embedding.shape[0], self.hidden_num), dtype=np.float32))\n",
    "            c_0 = torch.tensor(np.zeros((2, xs_embedding.shape[0], self.hidden_num), dtype=np.float32))\n",
    "        h_0 = h_0.to(self.device)\n",
    "        c_0 = c_0.to(self.device)\n",
    "        xs_embedding = xs_embedding.to(self.device)\n",
    "        ######定义模型######\n",
    "        hidden, (h_0, c_0) = self.lstm(xs_embedding, (h_0, c_0))\n",
    "        hidden_drop = self.dropout(hidden)\n",
    "        hidden_flatten = self.flatten(hidden_drop)\n",
    "        pre = self.linear(hidden_flatten)\n",
    "\n",
    "        return pre, (h_0, c_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70194fcf-7012-467a-b8bb-1cb1d92ef28b",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676c499f-551e-4a5a-9464-644f2e05f143",
   "metadata": {},
   "source": [
    "### 自动生成古诗:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70bd6aa6-6e97-4f73-8629-abbf13184f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poetry_auto():\n",
    "    result = \"\"\n",
    "    word_index = np.random.randint(0, word_size, 1)[0]\n",
    "\n",
    "    result += index_2_word[word_index]\n",
    "    h_0 = torch.tensor(np.zeros((2, 1, hidden_num), dtype=np.float32))\n",
    "    c_0 = torch.tensor(np.zeros((2, 1, hidden_num), dtype=np.float32))\n",
    "\n",
    "    for i in range(31):\n",
    "        word_embedding = torch.tensor(w1[word_index][None][None])\n",
    "        pre, (h_0, c_0) = model(word_embedding, h_0, c_0)\n",
    "        word_index = int(torch.argmax(pre))\n",
    "        result += index_2_word[word_index]\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33901082-0b12-4a85-a523-0b9745668be9",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd04be6a-9eaa-440b-9ea3-25a4f9a6eb48",
   "metadata": {},
   "source": [
    "### 藏头诗生成:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75beb453-4ced-48cb-867d-9d3a862f99ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poetry_acrostic():\n",
    "    input_text = input(\"请输入四个汉字：\")[:4]\n",
    "    result = \"\"\n",
    "    punctuation_list = [\"，\", \"。\", \"，\", \"。\"]\n",
    "    for i in range(4):\n",
    "        result += input_text[i]\n",
    "        h_0 = torch.tensor(np.zeros((2, 1, hidden_num), dtype=np.float32))\n",
    "        c_0 = torch.tensor(np.zeros((2, 1, hidden_num), dtype=np.float32))\n",
    "        word = input_text[i]\n",
    "        for j in range(6):\n",
    "            word_index = word_2_index[word]\n",
    "            word_embedding = torch.tensor(w1[word_index][None][None])\n",
    "            pre , (h_0,c_0) = model(word_embedding,h_0,c_0)\n",
    "            word = word_2_index[int(torch.argmax(pre))]\n",
    "            result += word\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc23dabc-7c12-453c-b3ce-f2d66f10b5bc",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581610cc-ea20-4484-93cb-e01d3c4048f4",
   "metadata": {},
   "source": [
    "### 主函数: 定义参数, 模型, 优化器, 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8638fa48-fbc5-44fa-b127-900567d1d012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:7.707\n",
      "舠，，，，，，，，。，，，，，，，，，，，，。，，，，。，，。，\n",
      "loss:6.962\n",
      "则，。。，。。，，。。。。，，。。，。，。。。，。。，。，，，。\n",
      "loss:6.800\n",
      "立，。，，。。。，。，，，。，。。。。。。。。。。。，，。。，。\n",
      "loss:6.725\n",
      "窅。。，有，，有。有，。。，色，色，有，有。，，，有，色，，有，\n",
      "loss:6.644\n",
      "马山花花花，山山山，花海，山山山山山花花，山花，花山山山山花花，\n",
      "loss:6.583\n",
      "壶山山山山，山，山山山，山山，山海，山山，山，山山海，山山海，山\n",
      "loss:6.414\n",
      "程山风风山无，，风，，，，，，，，，，，，，，，，，，，，，，，\n",
      "loss:6.292\n",
      "甲有三海不，，山，，，，，，，，，天。。山。一。。，一。，，，，\n",
      "loss:6.163\n",
      "半色三三一天，，来一一。一一。。。一一山。。。一一。。。一一。。\n",
      "loss:6.110\n",
      "双帆三路六水，，一山山一天。一一。山一无天。，一山一无无。。山一\n",
      "loss:6.051\n",
      "缘门三光斗，海天，一一，一天。一天。一一。一一。一一。一一。一一\n",
      "loss:5.956\n",
      "开得三十斗斗，一山不一一。一山一来。一一。一一水，一风一人。一来\n",
      "loss:5.864\n",
      "嗤台下车自斗险，一山一山人色。一山一一一。一山一人水。一来一似人\n",
      "loss:5.792\n",
      "陈榜风车斗天，天山不一一水。一来风来不人，一风一子一水。海山一人\n",
      "loss:5.737\n",
      "慵门由气千海菲，一花一山一天。一来一花一天。一山一年一水，一来一\n",
      "loss:5.676\n",
      "人人风风三远，海山一山一。金风一有无，一山一山不来。一来一有一无\n",
      "loss:5.601\n",
      "优今极树海有，萧山一山一青。一山风山一无。海山一里天时。一来一山\n",
      "loss:5.574\n",
      "弼门十草政苇阳，一山衔子一浮，。山烟垒一人。一来一声不节。一来一\n",
      "loss:5.532\n",
      "慧门十气斗注开，匹山无榔一水花。一山一山一人，一山一人频心。一来\n",
      "loss:5.509\n",
      "幔得下气接峰菲，砌霄一咏一山中。海时一望不来，海山烟山无春。一山\n",
      "loss:5.474\n",
      "槟阙喧光海告，一山似光一天。一天一来一来来，一来一风一无来。一山\n",
      "loss:5.408\n",
      "静台风光自生，匹下一光一天。一山风时一水，一里应花不公。一是一声\n",
      "loss:5.378\n",
      "晨蛇红山六重垧，瘴下一阁旧水中。海来一影一无花，一山遥风一云花。\n",
      "loss:5.333\n",
      "迥色烟路三有，东山一阁一天。一来一山一无。一来一人一无。海家一人\n",
      "loss:5.311\n",
      "斜台花气政澄，一山无阁一水。一山风风不水。一来一人一无，一山一处\n",
      "loss:5.268\n",
      "巧浪下车都重，一山清阁一公。一教一时一公。一教如头一野，海里一影\n",
      "loss:5.251\n",
      "逢台三人斗林天，一门一气一天端。白是一年知水，一山一复无天。未来\n",
      "loss:5.227\n",
      "穿寂下结接三天，破山隐咏如天林。一来一影一人客，一风一山一沧公。\n",
      "loss:5.216\n",
      "蜂门高构不天宣，一到天里一水心。一来一山不时老，，知一年初不花。\n",
      "loss:5.204\n",
      "术门走光香氏垧，一见一咏兴短仙。海阳不头一无绿，一来风年一大仙。\n",
      "loss:5.188\n",
      "逗门高结斗峰城，瓮贲台子一相林。但来一山谁时渡，争山一年一公青。\n",
      "loss:5.137\n",
      "提果高踞政自枝，振山清咏一短心。有是一年三无世，万月烟风一禅仙。\n",
      "loss:5.118\n",
      "推得觉光都氏宣，匹然台气见天心。一是一山无无在，一家烟处一浮时。\n",
      "loss:5.100\n",
      "宴门西光接峰堧，三章新山一诗端。天山一人无无有，未酒应处有园花。\n",
      "loss:5.041\n",
      "板门山光乘可中，瘴章编花旧节台。满山一年秋无，一来秋声无紫仙，一\n",
      "loss:5.049\n",
      "跋门筛结政绩垧，砌山编气一天羊。一风一年无野楣，檀梦遥处不沧仙。\n",
      "loss:5.035\n",
      "冉门不屭香其宣，振棠风子一盘心。海是不年不紫渡，一山一声不春花。\n",
      "loss:5.084\n",
      "眠门十光接绩垧，一光清阁一一长。一阁一头来多处，争山遥劚一禅仙。\n",
      "loss:5.069\n",
      "琳榜下传量氏履，砌到黄阁一并心。天知风觉无无在，愿里一人一云花。\n",
      "loss:4.973\n",
      "估门下光政冈席，甘岭烟咏统短公。木是一年无野会，一耳遥日共不花。\n",
      "loss:4.984\n",
      "墓门十光三露先，一光花咏一天莲。一是一教无野有，争山遥山未朦青。\n",
      "loss:4.925\n",
      "拘榜喧传接难先，砌棠编声一天公。遥今一藤知无在，一山遥风一人青。\n",
      "loss:4.905\n",
      "菩兢喧涧列莽先，砌马台花一染璃。此是一日清无在，一来深日一大花。\n",
      "loss:4.940\n",
      "骑女烟路乡铁婢，雕挛呵护色芙中。海是一风清无线，一指一时一留寒。\n",
      "loss:4.860\n",
      "犯门筛车日绩城，三山一护旧兰新。赤是一年天无在，海梦一风一沧青。\n",
      "loss:4.844\n",
      "经身秋雨度生城，一爱三花一短天。他阁揽教回时客，更山遥人一阳青。\n",
      "loss:4.809\n",
      "样门宝辣接绩垧，砌下编阁色染衣。白是一年天清林。海是一处无敷里，\n",
      "loss:4.771\n",
      "卜挤盘光六铁先，瘴章编气吐诗春。海上一年清无夜，黄来秋公一不来。\n",
      "loss:4.760\n",
      "邑帆贔物总峰垧，一霄此阁一诗屏。赤露一层知知后，残落天人一疑羹。\n",
      "loss:4.733\n",
      "篇暄童传姓氏先，文章台水吐青来。遥知一时知圣妄，一及一劚一大青。\n",
      "loss:4.728\n",
      "第璃猊推又注枝，匹章台阁水战春。怪是盛辔无时夜，万里一人共阳青。\n",
      "loss:4.629\n",
      "褒门高车政林垧，愿灵此阁一染屏。澄是盛舒频染供，愿报烟风一仔屏。\n",
      "loss:4.663\n",
      "慰得下结政绩韬，愿棠一咏色短屏。澄蚩一风驯田化，愿易遥风一短林，\n",
      "loss:4.629\n",
      "并茫风路乡注室，一霄载骨色娉挠。白是盛辔循番处，一绕烟瓠尚公仙。\n",
      "loss:4.599\n",
      "屠门下车看绩垧，无鬟一咏一染衣。海是海有来染醲，愿枝风劚尚仔中。\n",
      "loss:4.563\n",
      "渺宕淑气都难阳，软门盛岭色战场。白知盛德来醉会，一遣我村勒玉青。\n",
      "loss:4.485\n",
      "造门下光政萧宣，一凭衣溪吐染栖。铁阁一盏频染绿，白天一验一人家。\n",
      "loss:4.462\n",
      "垧兆喧传姓氏先，文章台根吐青深。家关揽路天无在，白月如日牛肯衣。\n",
      "loss:4.394\n",
      "弛兢喧檄渡氏，砌下又阁疑染蓉。菜甲一年初沙谙，愿得安验一仔中。笋\n",
      "loss:4.370\n",
      "尾余繁光认无茝，一棠无咏号虎莲。好里背路知无阔，愿鞭疑家一仔青。\n",
      "loss:4.384\n",
      "棱鱼腻屭挥画新，每穰芋叶焉行公。大寞独风藏心翠，重来文劚尚未笼。\n",
      "loss:4.360\n",
      "势花三柳自道涯，甡然天近一二端。十儿不随铭黄线，一乡应香寸槛钱。\n",
      "loss:4.335\n",
      "格门下结接林垧，砌下编篱作短屏。菜甲一年频音醪，愿甲卧竹拥觳脐。\n",
      "loss:4.203\n",
      "廷得下车并溶溶，小阑载国下深层。读甲一影偕舍渡，笋恭恒河死涛花。\n",
      "loss:4.167\n",
      "佳迷宿逦耕印熙，一步巅榔赋成公。天是一斜谁能老，万竿寒声一阳沉。\n",
      "loss:4.149\n",
      "忉蛇腻上谪点森，脂下宜手展青莲。雄是盛主留浏绿，只妍阵时一战窝。\n",
      "loss:4.109\n",
      "踵门下结耸绩垧，砌下编篱作毋挠。怒令如舒驯奋绿，拍奏阵洲无阳赠，\n",
      "loss:4.024\n",
      "男瀛风处度生阳，匹落盛篱似战场。白是一回金色夜，万梦遥槎汉疑香。\n",
      "loss:4.027\n",
      "甘光觉路六千阳，匹碎红里旧战场。满阁纡萸循筋綯，笋酒遥望寸一听。\n",
      "loss:3.917\n",
      "畅鸡吞剑认卒溶，砌下又篱作忉挠。离途一舒频染醲，愿得弼时拥禅脐。\n",
      "loss:3.975\n",
      "访清隐樯政绩宣，砌下编篱作毋挠。白笑蒲辔循雕泉，故乡无劚一飕鷖。\n",
      "loss:3.932\n",
      "萤鱼下车列绝名，井碎珊瑚作珠屏。三家握辔凭浏浏，海翮无虱故壤长。\n",
      "loss:3.855\n",
      "篷怪猊床侍释音，筠亚芒拍韭本丰。三苞风年僧舍信，海心遥处一钓筹。\n",
      "loss:3.812\n",
      "野本由柳自芳菲，西步无光不夕台。怪知年鸡天此在，畅飙陆理无翠家。\n",
      "loss:3.728\n",
      "篱八别翻三脚开，功嶂一莱一更馨。香日不幸黄凝集，万臣，身是老有寒\n",
      "loss:3.786\n",
      "星本由岸斗四知，也庸划房见相滨。巡海枯睹辛饶湿，笋丝脱花拥禅扉。\n",
      "loss:3.717\n",
      "试茫爽气豁无沙，招砾一溪照战枪。白发蒲葱罗公在，一桑烟忆久园稽。\n",
      "loss:3.632\n",
      "酷香筛辣春溶溶，砌醅傍天入短屏。菜甲初舒频染醪，更耳心槎贡人葩。\n",
      "loss:3.579\n",
      "度迷古事度峰催，更携此溪荖楫长。荡阳纵有天清夜，漫鸡壁垒几头红。\n",
      "loss:3.535\n",
      "仅叶高结接溶伦，甘载聪篱鑱挠支。换今内优来骑浏，鹿省风安似福星。\n",
      "loss:3.456\n",
      "缦门筛辣春溶溶，瓮醅芒教咏短屏。菜甲初舒频染绿，笋鳞未劚尚留青。\n",
      "loss:3.405\n",
      "桴榜喧传姓氏先，文章台阁吐青莲。天教盛德色敷治，故遣王味重未深。\n",
      "loss:3.372\n",
      "齐朝古雄三绸缪，大步樊笼半作淳。采是对处秋兴处，万沾万香或一寒。\n",
      "loss:3.376\n",
      "摘鱼腻极不斗齩，甘棠载咏入诗篇。澄蚩短语兼只妇，淹鞭熏起冠舜廷。\n",
      "loss:3.368\n",
      "科暄方法名冬舒，雕稻墨台芽诃茅。汲板绝时桅论水，教吴镇日一不飞。\n",
      "loss:3.286\n",
      "质得下樯列林垧，砌下编篱作短屏。菜甲初舒频染绿，笋恭沥河色定青。\n",
      "loss:3.204\n",
      "腻箐驯祥波峰寅，三霄异听白忉工。离蛇张兽鹏之化，愿鞭未劚尚留青。\n",
      "loss:3.130\n",
      "嵫日走割挥林垧，砌下编骨色琴。海是突辔循良泉，踏年金处尚洞箫。铁\n",
      "loss:3.151\n",
      "成耳极目闻载我，况蟾清西照子荧。夜来绛荔辛勤处，谨方脱壳射摇陶。\n",
      "loss:3.136\n",
      "叟日扶结接绝伦，砌下编篱作短屏。菜甲初舒呕翼獒，绝乡为虱却蛟家。\n",
      "loss:3.093\n",
      "屿仙含月庆若有，偶无匡叶问不妆。暴骇铿訇两鹏径，茧妍脱测附官联。\n",
      "loss:3.068\n",
      "颓门梅象不萧梦，鲤舶骸唤层头足。一清握手知公飞，腹缈扶莽一染思。\n",
      "loss:3.042\n",
      "宴歌坏邦控上头，井马罗山统舍耶。双螯客仆常敷化，笋留犹起重系肩。\n",
      "loss:3.083\n",
      "元滩螺前生笑累，入灵清胎飓母长。拜篷一献声虞翠，缦陌涛锐牙爪伤。\n",
      "loss:3.030\n",
      "荷身周气量余粟，愿乞骸前入诗足。菜甲初舒频染醲，云磬金肃尚女收。\n",
      "loss:2.957\n",
      "匏闪清国菊告中，鲲章台和暮井衣。剧阳一除秋首客，调竿民欲争霭凝。\n",
      "loss:2.853\n",
      "启璃浓露不千点，玳瑁筵酒玉几层。远种纤埃侵皓素，檀心普马御人狃。\n",
      "loss:2.856\n",
      "格义下旋雌远砧，愿乞骸骨还为狡。疏清揽辔知公志，愿更弼时重仔肩。\n",
      "loss:2.774\n",
      "椒璃浓露艳幽光，郑宅高芽两粉枪。白嫩蛎房调最滑，尘更佳浪此云泥。\n",
      "loss:2.827\n",
      "闽门御极量余眸，砌乞骸传作芙挠。酋亭初帆吠狂手。笋，舌蜜厌楼魛。\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_data, (w1, word_2_index, index_2_word) = train_vec(train_num=300)\n",
    "\n",
    "    batch_size = 32\n",
    "    epochs = 100\n",
    "    lr = 0.01\n",
    "    hidden_num = 128\n",
    "    word_size, embedding_num = w1.shape\n",
    "\n",
    "    dataset = Poetry_Dataset(w1, word_2_index, all_data)\n",
    "    dataloader = DataLoader(dataset, batch_size)\n",
    "\n",
    "    model = Poetry_Model_lstm(hidden_num, word_size, embedding_num)\n",
    "    model = model.to(model.device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        for batch_index, (batch_x_embedding, batch_y_index) in enumerate(dataloader):\n",
    "            model.train()\n",
    "            batch_x_embedding = batch_x_embedding.to(model.device)\n",
    "            batch_y_index = batch_y_index.to(model.device)\n",
    "\n",
    "            #模型预测\n",
    "            pre, _ = model(batch_x_embedding)\n",
    "            #计算损失\n",
    "            loss = model.cross_entropy(pre, batch_y_index.reshape(-1))\n",
    "            # 梯度反传 , 梯度累加, 但梯度并不更新, 梯度是由优化器更新的\n",
    "            loss.backward()\n",
    "            # 使用优化器更新梯度\n",
    "            optimizer.step()\n",
    "            # 梯度清零\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if batch_index % 100 == 0:\n",
    "                # model.eval()\n",
    "                print(f\"loss:{loss:.3f}\")\n",
    "                print(generate_poetry_auto())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34487dd",
   "metadata": {},
   "source": [
    "## 描述一下你的模型：\n",
    "\n",
    "\n",
    "### 模型概述\n",
    "\n",
    "该模型是一个基于深度学习的古诗生成系统，它利用word2vec技术将汉字表示为词向量，并通过LSTM（长短期记忆网络）和一个全连接层来构建预测模型。通过训练，该模型能够根据给定的前一个字预测下一个字，从而生成古诗。\n",
    "\n",
    "### 模型细节\n",
    "\n",
    "1. **汉字表示（word2vec）**\n",
    "\n",
    "   - **输入**：汉字集合（例如，从大量古诗中提取的所有不重复汉字）。\n",
    "   - **处理**：使用word2vec算法将每个汉字转换为一个固定维度的词向量。这些词向量能够捕捉到汉字之间的语义关系，例如相似的汉字在向量空间中会彼此靠近。\n",
    "   - **输出**：一个汉字到词向量的映射表，以及每个汉字对应的词向量。\n",
    "\n",
    "2. **封装数据**\n",
    "\n",
    "   - **目的**：为了方便模型处理，需要将汉字及其对应的词向量封装成数据格式。\n",
    "   - **处理**：为每个汉字的词向量分配一个唯一的索引，并创建一个字典来存储这些索引和词向量的对应关系。同时，将古诗文本转换为一系列索引，以便模型能够读取和预测。\n",
    "   - **输出**：索引到词向量的映射字典，以及古诗文本转换为索引序列的数据集。\n",
    "\n",
    "3. **构建模型**\n",
    "\n",
    "   - **架构**：模型由一个LSTM层和一个全连接层组成。LSTM层负责捕捉序列数据中的长期依赖关系，而全连接层则用于将LSTM的输出转换为预测下一个字的概率分布。\n",
    "   - **输入**：前一个字的词向量（通过索引从字典中检索得到）。\n",
    "   - **输出**：下一个字的可能性的概率分布（通常是一个softmax层输出的概率向量）。\n",
    "\n",
    "4. **训练**\n",
    "\n",
    "   - **过程**：使用大量古诗文本作为训练数据，通过前向传播和反向传播算法来训练模型。在训练过程中，模型会尝试根据前一个字的词向量来预测下一个字的索引。\n",
    "   - **损失函数**：通常使用交叉熵损失函数来衡量模型预测的准确性，并通过梯度下降等优化算法来更新模型的权重。\n",
    "   - **终止条件**：训练过程会在达到预定的迭代次数、损失函数收敛到某个阈值或模型在验证集上的性能不再提升时终止。\n",
    "\n",
    "5. **生成古诗**\n",
    "\n",
    "   - **过程**：在训练完成后，模型可以根据给定的起始字或句子生成古诗。这通常是通过在模型中输入起始字的词向量，并反复使用模型的预测输出来生成下一个字来实现的。\n",
    "   - **控制**：可以通过设置生成古诗的长度、使用特定的起始字或句子以及调整生成过程中的随机性来控制生成的古诗的风格和内容。\n",
    "\n",
    "### 总结\n",
    "\n",
    "该模型利用word2vec将汉字表示为词向量，并通过LSTM和全连接层构建了一个能够预测下一个字的深度学习模型。通过训练，该模型能够生成符合古诗风格的文本。这种模型在文学创作、自然语言处理等领域具有广泛的应用前景。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
