{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r67y9UpchZ38"
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "本次作业是完成 NLP 当中一个简单的 task —— 语句分类（文本分类）\n",
    "\n",
    "给定一个语句，判断有没有恶意（负面标 1，证明标 0）\n",
    "\n",
    "请构建基于LSTM的网络结构，尽可能提高准确率，在验证集上获得至少80%准确率\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YrAlczfM_w6"
   },
   "source": [
    "### Download Dataset\n",
    "有三個文档，分別是 training_label.txt、training_nolabel.txt、testing_data.txt\n",
    "\n",
    "- training_label.txt：共200000条数据，有 label 的 training data，（句子配上 0 or 1，+++$+++ 只是分隔符號，不要理它）\n",
    "    - e.g., 1 +++$+++ are wtf ... awww thanks !\n",
    "\n",
    "- training_nolabel.txt：共1178614条数据，沒有 label 的 training data（只有句子），用來做 semi-supervised learning\n",
    "    - ex: hates being this burnt !! ouch\n",
    "\n",
    "- testing_data.txt：共200000条数据，你要判斷 testing data 裡面的句子是 0 or 1\n",
    "\n",
    "    >id,text\n",
    "\n",
    "    >0,my dog ate our dinner . no , seriously ... he ate it .\n",
    "\n",
    "    >1,omg last day sooon n of primary noooooo x im gona be swimming out of school wif the amount of tears am gona cry\n",
    "\n",
    "    >2,stupid boys .. they ' re so .. stupid !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8hDIokoP6464"
   },
   "outputs": [],
   "source": [
    "# this is for filtering the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fc143hSvNGr6"
   },
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ICDIhhgCY2-M"
   },
   "outputs": [],
   "source": [
    "# utils.py\n",
    "# 這個 block 用來先定義一些常用到的函数\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import  torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def load_training_data(path = 'data/training_label.txt'):\n",
    "    # 把 training 時需要的 data 讀進來\n",
    "    # 如果是 'training_label.txt'，需要讀取 label，如果是 'training_nolabel.txt'，不需要讀取 label\n",
    "    if 'training_label' in path:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines() # Return a list\n",
    "            lines = [line.strip('\\n').split(' ') for line in lines]\n",
    "        x = [line[2:] for line in lines]\n",
    "        y = [line[0] for line in lines]\n",
    "        return x,y\n",
    "    else:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            x = [line.strip('\\n').split(' ') for line in lines]\n",
    "        return x\n",
    "\n",
    "def load_testing_data(path = 'data/testing_data.txt'):\n",
    "    # 把 testing 時需要的 data 讀進來\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        X = [\"\".join(line.strip('\\n').split(',')[1:]).strip() for line in lines[1:]] # 读取的时候去掉第一行\n",
    "        X = [sen.split(' ') for sen in X] # 分词\n",
    "    return X\n",
    "\n",
    "def evaluation(outputs, labels):\n",
    "    #outputs => probability (float)\n",
    "    #labels => labels\n",
    "    outputs[outputs>=0.5] = 1\n",
    "    outputs[outputs<0.5] = 0\n",
    "    correct = torch.sum(torch.eq(outputs, labels)).item()\n",
    "    return correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYE8UYQsNIxM"
   },
   "source": [
    "### Train Word to Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cgGWaF8_2S3q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training data ...\n",
      "loading testing data ...\n",
      "saving model ...\n"
     ]
    }
   ],
   "source": [
    "# w2v.py\n",
    "# 這個 block 是用來訓練 word to vector 的 word embedding\n",
    "# 注意！這個 block 在訓練 word to vector 時是用 cpu，可能要花到 10 分鐘以上\n",
    "import os\n",
    "import sys\n",
    "from gensim.models import word2vec\n",
    "sys.path.append(os.pardir) #返回当前文件的父目录\n",
    "\n",
    "path_prefix = './'\n",
    "def train_word2vec(x):\n",
    "    # 訓練 word to vector 的 word embedding\n",
    "    model = word2vec.Word2Vec(x, vector_size=250, window=5, min_count=5, workers=12, epochs=10, sg=1)\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"loading training data ...\")\n",
    "    train_x, y = load_training_data('training_label.txt')\n",
    "    train_x_no_label = load_training_data('training_nolabel.txt')\n",
    "\n",
    "    print(\"loading testing data ...\")\n",
    "    test_x = load_testing_data('testing_data.txt')\n",
    "\n",
    "    # model = train_word2vec(train_x + train_x_no_label + test_x)\n",
    "    model = train_word2vec(train_x + test_x)\n",
    "    \n",
    "    print(\"saving model ...\")\n",
    "    # model.save(os.path.join(path_prefix, 'model/w2v_all.model'))\n",
    "    model.save(os.path.join(path_prefix, 'w2v_all.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wHLtS0wNR6w"
   },
   "source": [
    "### Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "CfGKiOitk5ob"
   },
   "outputs": [],
   "source": [
    "# preprocess.py\n",
    "# 這個 block 用來做 data 的預處理\n",
    "from torch import nn\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "class Preprocess():\n",
    "    def __init__(self, sentences, sen_len, w2v_path=\"./w2v.model\"):\n",
    "        self.w2v_path = w2v_path\n",
    "        self.sentences = sentences\n",
    "        self.sen_len = sen_len\n",
    "        self.idx2word = []\n",
    "        self.word2idx = {}\n",
    "        self.embedding_matrix = []\n",
    "    def get_w2v_model(self):\n",
    "        # 把之前訓練好的 word to vec 模型讀進來\n",
    "        self.embedding = Word2Vec.load(self.w2v_path)\n",
    "        self.embedding_dim = self.embedding.vector_size\n",
    "    def add_embedding(self, word):\n",
    "        # 把 word 加進 embedding，並賦予他一個隨機生成的 representation vector\n",
    "        # word 只會是 \"<PAD>\" 或 \"<UNK>\"\n",
    "        vector = torch.empty(1, self.embedding_dim)\n",
    "        torch.nn.init.uniform_(vector)\n",
    "        self.word2idx[word] = len(self.word2idx)\n",
    "        self.idx2word.append(word)\n",
    "        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)\n",
    "    def make_embedding(self, load=True):\n",
    "        print(\"Get embedding ...\")\n",
    "        # 取得訓練好的 Word2vec word embedding\n",
    "        if load:\n",
    "            print(\"loading word to vec model ...\")\n",
    "            self.get_w2v_model()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        # 製作一個 word2idx 的 dictionary\n",
    "        # 製作一個 idx2word 的 list\n",
    "        # 製作一個 word2vector 的 list\n",
    "        for i, word in enumerate(self.embedding.wv.index_to_key):\n",
    "            print('get words #{}'.format(i+1), end='\\r')\n",
    "            #e.g. self.word2index['he'] = 1 \n",
    "            #e.g. self.index2word[1] = 'he'\n",
    "            #e.g. self.vectors[1] = 'he' vector\n",
    "            # self.word2idx[word] = len(self.word2idx)\n",
    "            self.word2idx[word] = i\n",
    "            self.idx2word.append(word)\n",
    "            self.embedding_matrix.append(self.embedding.wv[word])\n",
    "        print('')\n",
    "        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n",
    "        # 將 \"<PAD>\" 跟 \"<UNK>\" 加進 embedding 裡面\n",
    "        self.add_embedding(\"<PAD>\")\n",
    "        self.add_embedding(\"<UNK>\")\n",
    "        print(\"total words: {}\".format(len(self.embedding_matrix)))\n",
    "        return self.embedding_matrix\n",
    "    def pad_sequence(self, sentence):\n",
    "        # 將每個句子變成一樣的長度\n",
    "        if len(sentence) > self.sen_len:\n",
    "            sentence = sentence[:self.sen_len]\n",
    "        else:\n",
    "            pad_len = self.sen_len - len(sentence)\n",
    "            for _ in range(pad_len):\n",
    "                sentence.append(self.word2idx[\"<PAD>\"])\n",
    "        assert len(sentence) == self.sen_len\n",
    "        return sentence\n",
    "    def sentence_word2idx(self):\n",
    "        # 把句子裡面的字轉成相對應的 index\n",
    "        sentence_list = []\n",
    "        for i, sen in enumerate(self.sentences):\n",
    "            print('sentence count #{}'.format(i+1), end='\\r')\n",
    "            sentence_idx = []\n",
    "            for word in sen:\n",
    "                if (word in self.word2idx.keys()):\n",
    "                    sentence_idx.append(self.word2idx[word])\n",
    "                else:\n",
    "                    sentence_idx.append(self.word2idx[\"<UNK>\"])\n",
    "            # 將每個句子變成一樣的長度\n",
    "            sentence_idx = self.pad_sequence(sentence_idx)\n",
    "            sentence_list.append(sentence_idx)\n",
    "        return torch.LongTensor(sentence_list)\n",
    "    def labels_to_tensor(self, y):\n",
    "        # 把 labels 轉成 tensor\n",
    "        y = [int(label) for label in y]\n",
    "        return torch.LongTensor(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WJB7go5NWL0"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XketwKs4lFfB"
   },
   "outputs": [],
   "source": [
    "# data.py\n",
    "# 实现了 dataset 所需要的 '__init__', '__getitem__', '__len__'\n",
    "# 好讓 dataloader 能使用\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "class TwDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    Expected data shape like:(data_num, data_len)\n",
    "    Data can be a list of numpy array or a list of lists\n",
    "    input data shape : (data_num, seq_len, feature_dim)\n",
    "    \n",
    "    __len__ will return the number of data\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.data = X\n",
    "        self.label = y\n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is None: return self.data[idx]\n",
    "        return self.data[idx], self.label[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNJ8xWIMNa2r"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZS6RJADulIq1"
   },
   "outputs": [],
   "source": [
    "# model.py\n",
    "# 這個 block 是要拿來訓練的模型，请构建基于LSTM的网络结构\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class LSTM_Net(nn.Module):\n",
    "    def __init__(self, embedding, embedding_dim, hidden_dim, num_layers, dropout=0.5, fix_embedding=True):\n",
    "        super(LSTM_Net, self).__init__()\n",
    "        # 製作 embedding layer\n",
    "        self.embedding = torch.nn.Embedding(embedding.size(0),embedding.size(1))\n",
    "        self.embedding.weight = torch.nn.Parameter(embedding)\n",
    "        # 是否將 embedding fix 住，如果 fix_embedding 為 False，在訓練過程中，embedding 也會跟著被訓練\n",
    "        self.embedding.weight.requires_grad = False if fix_embedding else True\n",
    "        self.embedding_dim = embedding.size(1)\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.lstm = nn.LSTM(input_size = embedding_dim, hidden_size = hidden_dim, num_layers = num_layers, batch_first=True)\n",
    "        self.classifier = nn.Sequential(nn.Dropout(dropout),\n",
    "                                        nn.Linear(hidden_dim, 1),\n",
    "                                        nn.Sigmoid())\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    def forward(self, inputs):\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "        inputs = self.embedding(inputs)\n",
    "        x, _ = self.lstm(inputs, None)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWlpEL0sNc10"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4QR4MMz-lR7i"
   },
   "outputs": [],
   "source": [
    "# train.py\n",
    "# 這個 block 是用來訓練模型的\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def training(batch_size, n_epoch, lr, model_dir, train, valid, model, device):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print('\\nstart training, parameter total:{}, trainable:{}\\n'.format(total, trainable))\n",
    "    model.train() # 將 model 的模式設為 train，這樣 optimizer 就可以更新 model 的參數\n",
    "    criterion = nn.BCELoss() # 定義損失函數，這裡我們使用 binary cross entropy loss\n",
    "    t_batch = len(train) \n",
    "    v_batch = len(valid) \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) # 將模型的參數給 optimizer，並給予適當的 learning rate\n",
    "    total_loss, total_acc, best_acc = 0, 0, 0\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        total_loss, total_acc = 0, 0\n",
    "        # 這段做 training\n",
    "        for i, (inputs, labels) in enumerate(train):\n",
    "            inputs = inputs.to(device, dtype=torch.long) # device 為 \"cuda\"，將 inputs 轉成 torch.cuda.LongTensor\n",
    "            labels = labels.to(device, dtype=torch.float) # device為 \"cuda\"，將 labels 轉成 torch.cuda.FloatTensor，因為等等要餵進 criterion，所以型態要是 float\n",
    "            optimizer.zero_grad() # 由於 loss.backward() 的 gradient 會累加，所以每次餵完一個 batch 後需要歸零\n",
    "            outputs = model(inputs) # 將 input 餵給模型\n",
    "            outputs = outputs.squeeze() # 去掉最外面的 dimension，好讓 outputs 可以餵進 criterion()\n",
    "            loss = criterion(outputs, labels) # 計算此時模型的 training loss\n",
    "            loss.backward() # 算 loss 的 gradient\n",
    "            optimizer.step() # 更新訓練模型的參數\n",
    "            correct = evaluation(outputs, labels) # 計算此時模型的 training accuracy\n",
    "            total_acc += (correct / batch_size)\n",
    "            total_loss += loss.item()\n",
    "            print('[ Epoch{}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(\n",
    "            \tepoch+1, i+1, t_batch, loss.item(), correct*100/batch_size), end='\\r')\n",
    "        print('\\nTrain | Loss:{:.5f} Acc: {:.3f}'.format(total_loss/t_batch, total_acc/t_batch*100))\n",
    "\n",
    "        # 這段做 validation\n",
    "        model.eval() # 將 model 的模式設為 eval，這樣 model 的參數就會固定住\n",
    "        with torch.no_grad():\n",
    "            total_loss, total_acc = 0, 0\n",
    "            for i, (inputs, labels) in enumerate(valid):\n",
    "                inputs = inputs.to(device, dtype=torch.long) # device 為 \"cuda\"，將 inputs 轉成 torch.cuda.LongTensor\n",
    "                labels = labels.to(device, dtype=torch.float) # device 為 \"cuda\"，將 labels 轉成 torch.cuda.FloatTensor，因為等等要餵進 criterion，所以型態要是 float\n",
    "                outputs = model(inputs) # 將 input 餵給模型\n",
    "                outputs = outputs.squeeze() # 去掉最外面的 dimension，好讓 outputs 可以餵進 criterion()\n",
    "                loss = criterion(outputs, labels) # 計算此時模型的 validation loss\n",
    "                correct = evaluation(outputs, labels) # 計算此時模型的 validation accuracy\n",
    "                total_acc += (correct / batch_size)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(\"Valid | Loss:{:.5f} Acc: {:.3f} \".format(total_loss/v_batch, total_acc/v_batch*100))\n",
    "            if total_acc > best_acc:\n",
    "                # 如果 validation 的結果優於之前所有的結果，就把當下的模型存下來以備之後做預測時使用\n",
    "                best_acc = total_acc\n",
    "                #torch.save(model, \"{}/val_acc_{:.3f}.model\".format(model_dir,total_acc/v_batch*100))\n",
    "                torch.save(model, \"{}/ckpt.model\".format(model_dir))\n",
    "                print('saving model with acc {:.3f}'.format(total_acc/v_batch*100))\n",
    "        print('-----------------------------------------------')\n",
    "        model.train() # 將 model 的模式設為 train，這樣 optimizer 就可以更新 model 的參數（因為剛剛轉成 eval 模式）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qF5YQrupNfCS"
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "2X2wkdAYxHYA"
   },
   "outputs": [],
   "source": [
    "# test.py\n",
    "# 這個 block 用來對 testing_data.txt 做預測\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def testing(batch_size, test_loader, model, device):\n",
    "    model.eval()\n",
    "    ret_output = []\n",
    "    with torch.no_grad():\n",
    "        for i, inputs in enumerate(test_loader):\n",
    "            inputs = inputs.to(device, dtype=torch.long)\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze()\n",
    "            outputs[outputs>=0.5] = 1 # 大於等於 0.5 為正面\n",
    "            outputs[outputs<0.5] = 0 # 小於 0.5 為負面\n",
    "            ret_output += outputs.int().tolist()\n",
    "    \n",
    "    return ret_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfnKj0KXNeoz"
   },
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "EztIWqCmlZof"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data ...\n",
      "Get embedding ...\n",
      "loading word to vec model ...\n",
      "get words #24694\n",
      "total words: 24696\n",
      "sentence count #200000\n",
      "start training, parameter total:6415351, trainable:241351\n",
      "\n",
      "[ Epoch1: 1407/1407 ] loss:0.435 acc:21.094 \n",
      "Train | Loss:0.49846 Acc: 75.041\n",
      "Valid | Loss:0.45482 Acc: 78.055 \n",
      "saving model with acc 78.055\n",
      "-----------------------------------------------\n",
      "[ Epoch2: 1407/1407 ] loss:0.314 acc:21.875 \n",
      "Train | Loss:0.44490 Acc: 79.041\n",
      "Valid | Loss:0.43537 Acc: 79.329 \n",
      "saving model with acc 79.329\n",
      "-----------------------------------------------\n",
      "[ Epoch3: 1407/1407 ] loss:0.439 acc:18.750 \n",
      "Train | Loss:0.42829 Acc: 80.068\n",
      "Valid | Loss:0.43912 Acc: 79.061 \n",
      "-----------------------------------------------\n",
      "[ Epoch4: 1407/1407 ] loss:0.378 acc:19.531 \n",
      "Train | Loss:0.41632 Acc: 80.748\n",
      "Valid | Loss:0.42638 Acc: 80.190 \n",
      "saving model with acc 80.190\n",
      "-----------------------------------------------\n",
      "[ Epoch5: 1407/1407 ] loss:0.414 acc:19.531 \n",
      "Train | Loss:0.40466 Acc: 81.320\n",
      "Valid | Loss:0.42176 Acc: 80.339 \n",
      "saving model with acc 80.339\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from gensim.models import word2vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 通過 torch.cuda.is_available() 的回傳值進行判斷是否有使用 GPU 的環境，如果有的話 device 就設為 \"cuda\"，沒有的話就設為 \"cpu\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 處理好各個 data 的路徑\n",
    "train_with_label = os.path.join(path_prefix, 'training_label.txt')\n",
    "train_no_label = os.path.join(path_prefix, 'training_nolabel.txt')\n",
    "testing_data = os.path.join(path_prefix, 'testing_data.txt')\n",
    "\n",
    "w2v_path = os.path.join(path_prefix, 'w2v_all.model') # 處理 word to vec model 的路徑\n",
    "\n",
    "# 定義句子長度、要不要固定 embedding、batch 大小、要訓練幾個 epoch、learning rate 的值、model 的資料夾路徑\n",
    "sen_len = 20\n",
    "fix_embedding = True # fix embedding during training\n",
    "batch_size = 128\n",
    "epoch = 5\n",
    "lr = 0.001\n",
    "# model_dir = os.path.join(path_prefix, 'model/') # model directory for checkpoint model\n",
    "model_dir = path_prefix # model directory for checkpoint model\n",
    "\n",
    "print(\"loading data ...\") # 把 'training_label.txt' 跟 'training_nolabel.txt' 讀進來\n",
    "train_x, y = load_training_data(train_with_label)\n",
    "train_x_no_label = load_training_data(train_no_label)\n",
    "\n",
    "# 對 input 跟 labels 做預處理\n",
    "preprocess = Preprocess(train_x, sen_len, w2v_path=w2v_path)\n",
    "embedding = preprocess.make_embedding(load=True)\n",
    "train_x = preprocess.sentence_word2idx()\n",
    "y = preprocess.labels_to_tensor(y)\n",
    "\n",
    "# 製作一個 model 的對象\n",
    "model = LSTM_Net(embedding, embedding_dim=250, hidden_dim=150, num_layers=1, dropout=0.5, fix_embedding=fix_embedding)\n",
    "model = model.to(device) # device為 \"cuda\"，model 使用 GPU 來訓練（餵進去的 inputs 也需要是 cuda tensor）\n",
    "\n",
    "# 把 data 分為 training data 跟 validation data（將一部份 training data 拿去當作 validation data）\n",
    "X_train, X_val, y_train, y_val = train_x[:180000], train_x[180000:], y[:180000], y[180000:]\n",
    "\n",
    "# 把 data 做成 dataset 供 dataloader 取用\n",
    "train_dataset = TwDataset(X=X_train, y=y_train)\n",
    "val_dataset = TwDataset(X=X_val, y=y_val)\n",
    "\n",
    "# 把 data 轉成 batch of tensors\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = True,\n",
    "                                            num_workers = 0)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = False,\n",
    "                                            num_workers = 0)\n",
    "\n",
    "# 開始訓練\n",
    "training(batch_size, epoch, lr, model_dir, train_loader, val_loader, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fQeaQNeNm3L"
   },
   "source": [
    "### Predict and Write to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "vFvjFQopxVrt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading testing data ...\n",
      "Get embedding ...\n",
      "loading word to vec model ...\n",
      "get words #24694\n",
      "total words: 24696\n",
      "sentence count #200000\n",
      "load model ...\n",
      "save csv ...\n",
      "Finish Predicting\n"
     ]
    }
   ],
   "source": [
    "# 開始測試模型並做預測\n",
    "batch_size = 128\n",
    "print(\"loading testing data ...\")\n",
    "test_x = load_testing_data(testing_data)\n",
    "preprocess = Preprocess(test_x, sen_len, w2v_path=w2v_path)\n",
    "embedding = preprocess.make_embedding(load=True)\n",
    "test_x = preprocess.sentence_word2idx()\n",
    "test_dataset = TwDataset(X=test_x, y=None)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = False,\n",
    "                                            num_workers = 0)\n",
    "print('\\nload model ...')\n",
    "model = torch.load(os.path.join(model_dir, 'ckpt.model'))\n",
    "outputs = testing(batch_size, test_loader, model, device)\n",
    "\n",
    "# 写到csv存档 \n",
    "tmp = pd.DataFrame({\"id\": [str(i) for i in range(len(test_x))], \"label\": outputs})\n",
    "print(\"save csv ...\")\n",
    "tmp.to_csv('predict.csv', index=False)\n",
    "print(\"Finish Predicting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fQeaQNeNm3L"
   },
   "source": [
    "### 请描述你搭建的RNN架构、训练过程和准确率如何"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fQeaQNeNm3L"
   },
   "source": [
    "### 回答："
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
