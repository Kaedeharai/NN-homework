{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Introduction to PyTorch\n",
    "\n",
    "You've written a lot of code in this assignment to provide a whole host of neural network functionality. Dropout, Batch Norm, and 2D convolutions are some of the workhorses of deep learning in computer vision. You've also worked hard to make your code efficient and vectorized.\n",
    "\n",
    "For the last part of this assignment, though, we're going to leave behind your beautiful codebase and instead migrate to one of two popular deep learning frameworks: in this instance, PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "## Why do we use deep learning frameworks?\n",
    "\n",
    "* 我们的代码现在可以在gpu上运行了!这将使我们的模型训练得更快。当使用像PyTorch、TensorFlow或Paddle这样的框架时，你可以为自己的自定义神经网络架构利用GPU的强大功能，而不必直接编写CUDA代码(这超出了本课程的范围)。\n",
    "* 在这门课上，我们希望你准备好在你的项目中使用这些框架中的一个，这样你就可以比手工编写每个你想要使用的特性更有效地进行实验。\n",
    "* 我们要你站在巨人的肩膀上！PyTorch、TensorFlow或Paddle都是很棒的框架，它们会让你的工作变得轻松很多，现在你已经理解了它们的精髓，你就可以自由使用它们了 :) \n",
    "* 最后，我们希望你能够接触到你在学术界或工业界可能遇到的那种深度学习代码。\n",
    "\n",
    "## What is PyTorch?\n",
    "\n",
    "PyTorch是一个用于在行为类似numpy ndarray的Tensor对象上执行动态计算图的系统。它附带了一个强大的自动微分引擎，消除了手动反向传播的需要。\n",
    "\n",
    "## How do I learn PyTorch?\n",
    "\n",
    "我们的一位前导师Justin Johnson为PyTorch制作了一个非常棒的教程(https://github.com/jcjohnson/pytorch-examples)。 \n",
    "\n",
    "你也可以在这里找到详细的[API文档](http://pytorch.org/docs/stable/index.html)。\n",
    "如果你有API文档没有解决的其他问题，[PyTorch论坛](https://discuss.pytorch.org/)是一个比StackOverflow更好的提问场所。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "这个作业有5个部分。您将学习PyTorch的三个不同抽象层次，这将帮助您更好地理解它，并为最终项目做好准备。\n",
    "\n",
    "1. Part I, Preparation: 我们将使用CIFAR-10数据集。\n",
    "2. Part II, Barebones PyTorch: **Abstraction level 1**, 我们将直接使用最低级别的PyTorch张量。\n",
    "3. Part III, PyTorch Module API: **Abstraction level 2**, 我们将使用 `nn.Module`模块来定义任意的神经网络架构。\n",
    "4. Part IV, PyTorch Sequential API: **Abstraction level 3**, 我们将使用`nn.Sequential`，非常方便地定义线性前馈网络。\n",
    "5. Part V, CIFAR-10开放挑战：请在CIFAR-10上实现你自己的网络，以获得尽可能高的精度。你可以尝试任何层，优化器，超参数或其他高级功能。越高的准确率得分越高。\n",
    "\n",
    "Here is a table of comparison:\n",
    "\n",
    "| API           | Flexibility | Convenience |\n",
    "|---------------|-------------|-------------|\n",
    "| Barebone      | High        | Low         |\n",
    "| `nn.Module`     | High        | Medium      |\n",
    "| `nn.Sequential` | Low         | High        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "# GPU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "USE_GPU = True\n",
    "dtype = torch.float32 # We will be using float throughout this tutorial.\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss.\n",
    "print_every = 100\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. Preparation\n",
    "\n",
    "现在，让我们加载CIFAR-10数据集。第一次这样做可能需要几分钟，但之后文件应该保持缓存状态。\n",
    "\n",
    "在作业的前几部分，我们必须编写自己的代码来下载CIFAR-10数据集，预处理它，并在小批量中遍历它;PyTorch为我们提供了方便的工具来自动化这个过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "NUM_TRAIN = 49000\n",
    "\n",
    "# torchvision.transforms 包提供了用于预处理数据和执行数据增强的工具；\n",
    "# 在这里，我们设置了一个变换，通过减去平均 RGB 值并除以每个 RGB 值的标准差来预处理数据；\n",
    "# 我们已经硬编码了平均值和标准差。\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "# 我们为每次拆分设置一个Dataset对象(train / val / test); 数据集每次加载一个训练示例，\n",
    "# 因此我们将每个数据集包装在一个DataLoader中，该DataLoader在数据集中迭代并形成小批。 \n",
    "# 我们将CIFAR-10训练集分为train集和val集，方法是向DataLoader传递一个Sampler对象，\n",
    "# 告诉它应该如何从底层数据集进行采样。 \n",
    "\n",
    "cifar10_train = dset.CIFAR10('./datasets', train=True, download=True,\n",
    "                             transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./datasets', train=True, download=True,\n",
    "                           transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./datasets', train=False, download=True, \n",
    "                            transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Barebones PyTorch\n",
    "\n",
    "PyTorch附带了高级api，可以帮助我们方便地定义模型体系结构，我们将在本教程的第二部分中介绍。在本节中，我们将从最基本的PyTorch元素开始，以便更好地理解自动升级引擎。在这个练习之后，您将会更加欣赏高级模型API。\n",
    "\n",
    "我们将从一个简单的全连接ReLU网络开始，该网络具有两个隐藏层，并且对CIFAR分类没有偏见。\n",
    "这个实现使用PyTorch张量上的操作来计算正向传递，并使用PyTorch autograd来计算梯度。理解每一行很重要，因为您将在示例之后编写更难的版本。\n",
    "\n",
    "当我们创建一个带有' requires_grad=True '的PyTorch张量时，涉及到这个张量的操作将不仅仅是计算值;他们还会在后台建立一个计算图，让我们可以很容易地通过图进行反向传播，计算一些张量关于下游损失的梯度。具体来说，如果x是一个带有' x.requires_grad == True'的张量，然后在反向传播' x.grad '将是另一个张量，保持x的梯度，相对于最后的标量损失。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "### PyTorch Tensors: Flatten Function\n",
    "PyTorch张量在概念上类似于numpy数组:它是一个n维的数字网格，和numpy一样，PyTorch提供了许多函数来有效地操作张量。作为一个简单的例子，我们在下面提供了一个flatten函数，它可以重塑图像数据，以便在完全连接的神经网络中使用。\n",
    "\n",
    "Recall that image data is typically stored in a Tensor of shape N x C x H x W, where:\n",
    "\n",
    "* N is the number of datapoints\n",
    "* C is the number of channels\n",
    "* H is the height of the intermediate feature map in pixels\n",
    "* W is the height of the intermediate feature map in pixels\n",
    "\n",
    "当我们在做像二维卷积这样的事情时，这是表示数据的正确方式，这需要对中间特征彼此相对位置的空间理解。然而，当我们使用完全连接的仿射层处理图像时，我们希望每个数据点由单个向量表示——分隔数据的不同通道、行和列不再有用。所以，我们使用“flatten”操作将每个表示的“C x H x W”值折叠成一个长向量。下面的flatten函数首先从给定的一批数据中读入N、C、H和W值，然后返回该数据的“View”。“View”类似于numpy的“reshape”方法:它将x的维度重塑为N x ??,在哪里? ?可以是任何值(在本例中，它将是C x H x W，但我们不需要显式地指定它)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before flattening:  tensor([[[[ 0,  1],\n",
      "          [ 2,  3],\n",
      "          [ 4,  5]]],\n",
      "\n",
      "\n",
      "        [[[ 6,  7],\n",
      "          [ 8,  9],\n",
      "          [10, 11]]]])\n",
      "After flattening:  tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "\n",
    "def test_flatten():\n",
    "    x = torch.arange(12).view(2, 1, 3, 2)\n",
    "    print('Before flattening: ', x)\n",
    "    print('After flattening: ', flatten(x))\n",
    "\n",
    "test_flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "### Barebones PyTorch: Two-Layer Network\n",
    "\n",
    "在这里，我们定义了一个函数two_layer_fc，它在一批图像数据上执行两层全连接ReLU网络的转发传递。在定义了正向传递之后，我们检查它不会崩溃，并且通过在网络中运行零来产生正确形状的输出。\n",
    "\n",
    "你不需要在这里编写任何代码，但是阅读和理解实现是很重要的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F  # useful stateless functions\n",
    "\n",
    "def two_layer_fc(x, params):\n",
    "    \"\"\"\n",
    "    A fully-connected neural networks; the architecture is:\n",
    "    NN is fully connected -> ReLU -> fully connected layer.\n",
    "    Note that this function only defines the forward pass; \n",
    "    PyTorch will take care of the backward pass for us.\n",
    "    \n",
    "    The input to the network will be a minibatch of data, of shape\n",
    "    (N, d1, ..., dM) where d1 * ... * dM = D. The hidden layer will have H units,\n",
    "    and the output layer will produce scores for C classes.\n",
    "    \n",
    "    Inputs:\n",
    "    - x: A PyTorch Tensor of shape (N, d1, ..., dM) giving a minibatch of\n",
    "      input data.\n",
    "    - params: A list [w1, w2] of PyTorch Tensors giving weights for the network;\n",
    "      w1 has shape (D, H) and w2 has shape (H, C).\n",
    "    \n",
    "    Returns:\n",
    "    - scores: A PyTorch Tensor of shape (N, C) giving classification scores for\n",
    "      the input data x.\n",
    "    \"\"\"\n",
    "    # first we flatten the image\n",
    "    x = flatten(x)  # shape: [batch_size, C x H x W]\n",
    "    \n",
    "    w1, w2 = params\n",
    "    \n",
    "    # Forward pass: compute predicted y using operations on Tensors. Since w1 and\n",
    "    # w2 have requires_grad=True, operations involving these Tensors will cause\n",
    "    # PyTorch to build a computational graph, allowing automatic computation of\n",
    "    # gradients. Since we are no longer implementing the backward pass by hand we\n",
    "    # don't need to keep references to intermediate values.\n",
    "    # you can also use `.clamp(min=0)`, equivalent to F.relu()\n",
    "    x = F.relu(x.mm(w1))\n",
    "    x = x.mm(w2)\n",
    "    return x\n",
    "    \n",
    "\n",
    "def two_layer_fc_test():\n",
    "    hidden_layer_size = 42\n",
    "    x = torch.zeros((64, 50), dtype=dtype)  # minibatch size 64, feature dimension 50\n",
    "    w1 = torch.zeros((50, hidden_layer_size), dtype=dtype)\n",
    "    w2 = torch.zeros((hidden_layer_size, 10), dtype=dtype)\n",
    "    scores = two_layer_fc(x, [w1, w2])\n",
    "    print(scores.size())  # you should see [64, 10]\n",
    "\n",
    "two_layer_fc_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barebones PyTorch: Three-Layer ConvNet\n",
    "\n",
    "在这里，您将完成函数three_layer_convnet的实现，该函数将执行三层卷积网络的正向传递。与上面一样，我们可以通过在网络中传递零来立即测试我们的实现。网络应具有以下架构:\n",
    "\n",
    "1. A convolutional layer (with bias) with `channel_1` filters, each with shape `KW1 x KH1`, and zero-padding of two\n",
    "2. ReLU nonlinearity\n",
    "3. A convolutional layer (with bias) with `channel_2` filters, each with shape `KW2 x KH2`, and zero-padding of one\n",
    "4. ReLU nonlinearity\n",
    "5. Fully-connected layer with bias, producing scores for C classes.\n",
    "\n",
    "注意，在我们的全连接层之后，我们这里没有**softmax activation**: 这是因为PyTorch的交叉熵损失为你执行了softmax激活，并通过绑定该步骤使计算更有效。\n",
    "\n",
    "**HINT**: For convolutions: http://pytorch.org/docs/stable/nn.html#torch.nn.functional.conv2d; pay attention to the shapes of convolutional filters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_layer_convnet(x, params):\n",
    "    \"\"\"\n",
    "    Performs the forward pass of a three-layer convolutional network with the\n",
    "    architecture defined above.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A PyTorch Tensor of shape (N, 3, H, W) giving a minibatch of images\n",
    "    - params: A list of PyTorch Tensors giving the weights and biases for the\n",
    "      network; should contain the following:\n",
    "      - conv_w1: PyTorch Tensor of shape (channel_1, 3, KH1, KW1) giving weights\n",
    "        for the first convolutional layer\n",
    "      - conv_b1: PyTorch Tensor of shape (channel_1,) giving biases for the first\n",
    "        convolutional layer\n",
    "      - conv_w2: PyTorch Tensor of shape (channel_2, channel_1, KH2, KW2) giving\n",
    "        weights for the second convolutional layer\n",
    "      - conv_b2: PyTorch Tensor of shape (channel_2,) giving biases for the second\n",
    "        convolutional layer\n",
    "      - fc_w: PyTorch Tensor giving weights for the fully-connected layer. Can you\n",
    "        figure out what the shape should be?\n",
    "      - fc_b: PyTorch Tensor giving biases for the fully-connected layer. Can you\n",
    "        figure out what the shape should be?\n",
    "    \n",
    "    Returns:\n",
    "    - scores: PyTorch Tensor of shape (N, C) giving classification scores for x\n",
    "    \"\"\"\n",
    "    conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b = params\n",
    "    scores = None\n",
    "    ################################################################################\n",
    "    # TODO: Implement the forward pass for the three-layer ConvNet.                #\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    x = torch.nn.functional.conv2d(x, conv_w1, conv_b1, stride=1, padding=2)\n",
    "    x = x.clamp(min = 0)\n",
    "    x = torch.nn.functional.conv2d(x, conv_w2, conv_b2, stride=1, padding=1)\n",
    "    x = x.clamp(min = 0)\n",
    "    x = flatten(x)\n",
    "    scores = x.mm(fc_w) + fc_b\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ################################################################################\n",
    "    #                                 END OF YOUR CODE                             #\n",
    "    ################################################################################\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在定义了上面ConvNet的向前传递之后，运行以下单元格来测试您的实现。\n",
    "\n",
    "当运行这个函数时，分数应该具有形状(64,10)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "pdf-ignore-input"
    ],
    "test": "barebones_output_shape"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "def three_layer_convnet_test():\n",
    "    x = torch.zeros((64, 3, 32, 32), dtype=dtype)  # minibatch size 64, image size [3, 32, 32]\n",
    "\n",
    "    conv_w1 = torch.zeros((6, 3, 5, 5), dtype=dtype)  # [out_channel, in_channel, kernel_H, kernel_W]\n",
    "    conv_b1 = torch.zeros((6,))  # out_channel\n",
    "    conv_w2 = torch.zeros((9, 6, 3, 3), dtype=dtype)  # [out_channel, in_channel, kernel_H, kernel_W]\n",
    "    conv_b2 = torch.zeros((9,))  # out_channel\n",
    "\n",
    "    # you must calculate the shape of the tensor after two conv layers, before the fully-connected layer\n",
    "    fc_w = torch.zeros((9 * 32 * 32, 10))\n",
    "    fc_b = torch.zeros(10)\n",
    "\n",
    "    scores = three_layer_convnet(x, [conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b])\n",
    "    print(scores.size())  # you should see [64, 10]\n",
    "three_layer_convnet_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barebones PyTorch: Initialization\n",
    "让我们编写两个实用程序方法来初始化模型的权重矩阵。\n",
    "\n",
    "- `random_weight(shape)` 用Kaiming归一化方法初始化一个权张量。\n",
    "- `zero_weight(shape)` 初始化一个全为0的权值张量。用于实例化偏置参数。\n",
    "\n",
    "The `random_weight` function uses the Kaiming normal initialization method, described in:\n",
    "\n",
    "He et al, *Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification*, ICCV 2015, https://arxiv.org/abs/1502.01852"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0097,  0.5370, -0.4325,  0.4246,  0.6793],\n",
       "        [-0.6077,  2.1023, -1.5932, -0.5714, -0.7334],\n",
       "        [-0.7410,  0.7438, -0.4543, -1.5367,  0.3243]], device='cuda:0',\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def random_weight(shape):\n",
    "    \"\"\"\n",
    "    Create random Tensors for weights; setting requires_grad=True means that we\n",
    "    want to compute gradients for these Tensors during the backward pass.\n",
    "    We use Kaiming normalization: sqrt(2 / fan_in)\n",
    "    \"\"\"\n",
    "    if len(shape) == 2:  # FC weight\n",
    "        fan_in = shape[0]\n",
    "    else:\n",
    "        fan_in = np.prod(shape[1:]) # conv weight [out_channel, in_channel, kH, kW]\n",
    "    # randn is standard normal distribution generator. \n",
    "    w = torch.randn(shape, device=device, dtype=dtype) * np.sqrt(2. / fan_in)\n",
    "    w.requires_grad = True\n",
    "    return w\n",
    "\n",
    "def zero_weight(shape):\n",
    "    return torch.zeros(shape, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "# create a weight of shape [3 x 5]\n",
    "# you should see the type `torch.cuda.FloatTensor` if you use GPU. \n",
    "# Otherwise it should be `torch.FloatTensor`\n",
    "random_weight((3, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barebones PyTorch: Check Accuracy\n",
    "当训练模型时，我们将使用以下函数来检查我们的模型在训练集或验证集上的准确性。\n",
    "\n",
    "在检查精确度时，我们不需要计算任何梯度;因此，当我们计算分数时，我们不需要PyTorch为我们构建计算图。为了防止图被构建，我们在' torch.no_grad() '上下文管理器下进行计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [],
   "source": [
    "def check_accuracy_part2(loader, model_fn, params):\n",
    "    \"\"\"\n",
    "    Check the accuracy of a classification model.\n",
    "    \n",
    "    Inputs:\n",
    "    - loader: A DataLoader for the data split we want to check\n",
    "    - model_fn: A function that performs the forward pass of the model,\n",
    "      with the signature scores = model_fn(x, params)\n",
    "    - params: List of PyTorch Tensors giving parameters of the model\n",
    "    \n",
    "    Returns: Nothing, but prints the accuracy of the model\n",
    "    \"\"\"\n",
    "    split = 'val' if loader.dataset.train else 'test'\n",
    "    print('Checking accuracy on the %s set' % split)\n",
    "    num_correct, num_samples = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.int64)\n",
    "            scores = model_fn(x, params)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f%%)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BareBones PyTorch: Training Loop\n",
    "We can now set up a basic training loop to train our network. We will train the model using stochastic gradient descent without momentum. We will use `torch.functional.cross_entropy` to compute the loss; you can [read about it here](http://pytorch.org/docs/stable/nn.html#cross-entropy).\n",
    "\n",
    "The training loop takes as input the neural network function, a list of initialized parameters (`[w1, w2]` in our example), and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [],
   "source": [
    "def train_part2(model_fn, params, learning_rate):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10.\n",
    "    \n",
    "    Inputs:\n",
    "    - model_fn: A Python function that performs the forward pass of the model.\n",
    "      It should have the signature scores = model_fn(x, params) where x is a\n",
    "      PyTorch Tensor of image data, params is a list of PyTorch Tensors giving\n",
    "      model weights, and scores is a PyTorch Tensor of shape (N, C) giving\n",
    "      scores for the elements in x.\n",
    "    - params: List of PyTorch Tensors giving weights for the model\n",
    "    - learning_rate: Python scalar giving the learning rate to use for SGD\n",
    "    \n",
    "    Returns: Nothing\n",
    "    \"\"\"\n",
    "    for t, (x, y) in enumerate(loader_train):\n",
    "        # Move the data to the proper device (GPU or CPU)\n",
    "        x = x.to(device=device, dtype=dtype)\n",
    "        y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "        # Forward pass: compute scores and loss\n",
    "        scores = model_fn(x, params)\n",
    "        loss = F.cross_entropy(scores, y)\n",
    "\n",
    "        # Backward pass: PyTorch figures out which Tensors in the computational\n",
    "        # graph has requires_grad=True and uses backpropagation to compute the\n",
    "        # gradient of the loss with respect to these Tensors, and stores the\n",
    "        # gradients in the .grad attribute of each Tensor.\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters. We don't want to backpropagate through the\n",
    "        # parameter updates, so we scope the updates under a torch.no_grad()\n",
    "        # context manager to prevent a computational graph from being built.\n",
    "        with torch.no_grad():\n",
    "            for w in params:\n",
    "                w -= learning_rate * w.grad\n",
    "\n",
    "                # Manually zero the gradients after running the backward pass\n",
    "                w.grad.zero_()\n",
    "\n",
    "        if t % print_every == 0:\n",
    "            print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "            check_accuracy_part2(loader_val, model_fn, params)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BareBones PyTorch: Train a Two-Layer Network\n",
    "现在我们准备运行训练循环。我们需要显式地为全连接权值' w1 '和' w2 '分配张量。\n",
    "\n",
    "CIFAR的每个小批都有64个示例，因此张量形状为'[64,3,32,32]'。\n",
    "\n",
    "扁平化后，' x '形状应该是'[64,3 * 32 * 32]'。这将是' w1 '的第一个维度的大小。\n",
    "' w1 '的第二个维度是隐藏层的大小，它也将是' w2 '的第一个维度。\n",
    "\n",
    "最后，网络的输出是一个表示10个类的概率分布的10维向量。\n",
    "\n",
    "你不需要调优任何超参数，但你应该看到准确性超过40%训练后的一个epoch。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 3.7337\n",
      "Checking accuracy on the val set\n",
      "Got 133 / 1000 correct (13.30%)\n",
      "\n",
      "Iteration 100, loss = 2.4485\n",
      "Checking accuracy on the val set\n",
      "Got 290 / 1000 correct (29.00%)\n",
      "\n",
      "Iteration 200, loss = 2.0484\n",
      "Checking accuracy on the val set\n",
      "Got 378 / 1000 correct (37.80%)\n",
      "\n",
      "Iteration 300, loss = 1.9386\n",
      "Checking accuracy on the val set\n",
      "Got 396 / 1000 correct (39.60%)\n",
      "\n",
      "Iteration 400, loss = 1.9444\n",
      "Checking accuracy on the val set\n",
      "Got 428 / 1000 correct (42.80%)\n",
      "\n",
      "Iteration 500, loss = 2.1081\n",
      "Checking accuracy on the val set\n",
      "Got 406 / 1000 correct (40.60%)\n",
      "\n",
      "Iteration 600, loss = 1.4009\n",
      "Checking accuracy on the val set\n",
      "Got 436 / 1000 correct (43.60%)\n",
      "\n",
      "Iteration 700, loss = 1.7375\n",
      "Checking accuracy on the val set\n",
      "Got 455 / 1000 correct (45.50%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_size = 4000\n",
    "learning_rate = 1e-2\n",
    "\n",
    "w1 = random_weight((3 * 32 * 32, hidden_layer_size))\n",
    "w2 = random_weight((hidden_layer_size, 10))\n",
    "\n",
    "train_part2(two_layer_fc, [w1, w2], learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BareBones PyTorch: Training a ConvNet\n",
    "\n",
    "在下文中，您应该使用上面定义的函数在CIFAR上训练三层卷积网络。网络应具有以下架构:\n",
    "\n",
    "1. Convolutional layer (with bias) with 32 5x5 filters, with zero-padding of 2\n",
    "2. ReLU\n",
    "3. Convolutional layer (with bias) with 16 3x3 filters, with zero-padding of 1\n",
    "4. ReLU\n",
    "5. Fully-connected layer (with bias) to compute scores for 10 classes\n",
    "\n",
    "你应该使用上面定义的' random_weight '函数初始化你的权重矩阵，你应该使用上面定义的' zero_weight '函数初始化你的偏置向量。\n",
    "\n",
    "你不需要调优任何超参数，但如果一切正常，你应该在一个epoch后达到42%以上的精度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "test": "barebones_accuracy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 3.6305\n",
      "Checking accuracy on the val set\n",
      "Got 96 / 1000 correct (9.60%)\n",
      "\n",
      "Iteration 100, loss = 1.9413\n",
      "Checking accuracy on the val set\n",
      "Got 341 / 1000 correct (34.10%)\n",
      "\n",
      "Iteration 200, loss = 1.7457\n",
      "Checking accuracy on the val set\n",
      "Got 373 / 1000 correct (37.30%)\n",
      "\n",
      "Iteration 300, loss = 1.6714\n",
      "Checking accuracy on the val set\n",
      "Got 423 / 1000 correct (42.30%)\n",
      "\n",
      "Iteration 400, loss = 1.9223\n",
      "Checking accuracy on the val set\n",
      "Got 405 / 1000 correct (40.50%)\n",
      "\n",
      "Iteration 500, loss = 1.3667\n",
      "Checking accuracy on the val set\n",
      "Got 437 / 1000 correct (43.70%)\n",
      "\n",
      "Iteration 600, loss = 1.4911\n",
      "Checking accuracy on the val set\n",
      "Got 469 / 1000 correct (46.90%)\n",
      "\n",
      "Iteration 700, loss = 1.5586\n",
      "Checking accuracy on the val set\n",
      "Got 464 / 1000 correct (46.40%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-3\n",
    "\n",
    "channel_1 = 32\n",
    "channel_2 = 16\n",
    "\n",
    "conv_w1 = None\n",
    "conv_b1 = None\n",
    "conv_w2 = None\n",
    "conv_b2 = None\n",
    "fc_w = None\n",
    "fc_b = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: Initialize the parameters of a three-layer ConvNet.                    #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "conv_w1 = random_weight((channel_1, 3, 5, 5))\n",
    "conv_b1 = zero_weight(channel_1)\n",
    "conv_w2 = random_weight((channel_2, channel_1, 3, 3))\n",
    "conv_b2 = zero_weight(channel_2)\n",
    "fc_w = random_weight((32*32*channel_2, 10))\n",
    "fc_b = zero_weight(10)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "\n",
    "params = [conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b]\n",
    "train_part2(three_layer_convnet, params, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III. PyTorch Module API\n",
    "\n",
    "Barebone PyTorch要求我们手动跟踪所有的参数张量。这对于只有几个张量的小型网络来说是很好的，但是在较大的网络中跟踪几十或几百个张量将非常不方便和容易出错。\n",
    "\n",
    "PyTorch provides the `nn.Module` API for you to define arbitrary network architectures, while tracking every learnable parameters for you. In Part II, we implemented SGD ourselves. PyTorch also provides the `torch.optim` package that implements all the common optimizers, such as RMSProp, Adagrad, and Adam. It even supports approximate second-order methods like L-BFGS! You can refer to the [doc](http://pytorch.org/docs/master/optim.html) for the exact specifications of each optimizer.\n",
    "\n",
    "To use the Module API, follow the steps below:\n",
    "\n",
    "1. Subclass `nn.Module`. Give your network class an intuitive name like `TwoLayerFC`. \n",
    "\n",
    "2. In the constructor `__init__()`, define all the layers you need as class attributes. Layer objects like `nn.Linear` and `nn.Conv2d` are themselves `nn.Module` subclasses and contain learnable parameters, so that you don't have to instantiate the raw tensors yourself. `nn.Module` will track these internal parameters for you. Refer to the [doc](http://pytorch.org/docs/master/nn.html) to learn more about the dozens of builtin layers. **Warning**: don't forget to call the `super().__init__()` first!\n",
    "\n",
    "3. In the `forward()` method, define the *connectivity* of your network. You should use the attributes defined in `__init__` as function calls that take tensor as input and output the \"transformed\" tensor. Do *not* create any new layers with learnable parameters in `forward()`! All of them must be declared upfront in `__init__`. \n",
    "\n",
    "After you define your Module subclass, you can instantiate it as an object and call it just like the NN forward function in part II.\n",
    "\n",
    "### Module API: Two-Layer Network\n",
    "Here is a concrete example of a 2-layer fully connected network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "class TwoLayerFC(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        # assign layer objects to class attributes\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        # nn.init package contains convenient initialization methods\n",
    "        # http://pytorch.org/docs/master/nn.html#torch-nn-init \n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # forward always defines connectivity\n",
    "        x = flatten(x)\n",
    "        scores = self.fc2(F.relu(self.fc1(x)))\n",
    "        return scores\n",
    "\n",
    "def test_TwoLayerFC():\n",
    "    input_size = 50\n",
    "    x = torch.zeros((64, input_size), dtype=dtype)  # minibatch size 64, feature dimension 50\n",
    "    model = TwoLayerFC(input_size, 42, 10)\n",
    "    scores = model(x)\n",
    "    print(scores.size())  # you should see [64, 10]\n",
    "test_TwoLayerFC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module API: Three-Layer ConvNet\n",
    "现在轮到你实现一个3层ConvNet，然后是一个完全连接的层。网络架构应与第二部分相同:\n",
    "\n",
    "1. Convolutional layer with `channel_1` 5x5 filters with zero-padding of 2\n",
    "2. ReLU\n",
    "3. Convolutional layer with `channel_2` 3x3 filters with zero-padding of 1\n",
    "4. ReLU\n",
    "5. Fully-connected layer to `num_classes` classes\n",
    "\n",
    "You should initialize the weight matrices of the model using the Kaiming normal initialization method.\n",
    "\n",
    "**HINT**: http://pytorch.org/docs/stable/nn.html#conv2d\n",
    "\n",
    "After you implement the three-layer ConvNet, the `test_ThreeLayerConvNet` function will run your implementation; it should print `(64, 10)` for the shape of the output scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "test": "module_output_shape"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "class ThreeLayerConvNet(nn.Module):\n",
    "    def __init__(self, in_channel, channel_1, channel_2, num_classes):\n",
    "        super().__init__()\n",
    "        ########################################################################\n",
    "        # TODO: Set up the layers you need for a three-layer ConvNet with the  #\n",
    "        # architecture defined above.                                          #\n",
    "        ########################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, channel_1, 5, 1, 2)\n",
    "        self.conv2 = nn.Conv2d(channel_1, channel_2, 3, 1, 1)\n",
    "        self.fc = nn.Linear(channel_2*32*32, num_classes)\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ########################################################################\n",
    "        #                          END OF YOUR CODE                            #       \n",
    "        ########################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        scores = None\n",
    "        ########################################################################\n",
    "        # TODO: Implement the forward function for a 3-layer ConvNet. you      #\n",
    "        # should use the layers you defined in __init__ and specify the        #\n",
    "        # connectivity of those layers in forward()                            #\n",
    "        ########################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = flatten(x)\n",
    "        scores = self.fc(x)\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ########################################################################\n",
    "        #                             END OF YOUR CODE                         #\n",
    "        ########################################################################\n",
    "        return scores\n",
    "\n",
    "\n",
    "def test_ThreeLayerConvNet():\n",
    "    x = torch.zeros((64, 3, 32, 32), dtype=dtype)  # minibatch size 64, image size [3, 32, 32]\n",
    "    model = ThreeLayerConvNet(in_channel=3, channel_1=12, channel_2=8, num_classes=10)\n",
    "    scores = model(x)\n",
    "    print(scores.size())  # you should see [64, 10]\n",
    "test_ThreeLayerConvNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module API: Check Accuracy\n",
    "给定验证集或测试集，我们可以检验神经网络的分类精度。\n",
    "\n",
    "这个版本与第二部分的版本略有不同。您不再手动传递参数了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy_part34(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module API: Training Loop\n",
    "We also use a slightly different training loop. Rather than updating the values of the weights ourselves, we use an Optimizer object from the `torch.optim` package, which abstract the notion of an optimization algorithm and provides implementations of most of the algorithms commonly used to optimize neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part34(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    使用PyTorch模块API在CIFAR-10上训练模型。\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                check_accuracy_part34(loader_val, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module API: Train a Two-Layer Network\n",
    "现在我们准备运行训练循环。与第二部分相比，我们不再显式分配参数张量。\n",
    "\n",
    "简单地将输入大小、隐藏层大小和类的数量(即输出大小)传递给' TwoLayerFC '的构造函数。\n",
    "\n",
    "你还需要定义一个优化器来跟踪' TwoLayerFC '中的所有可学习参数。\n",
    "\n",
    "您不需要调优任何超参数，但是您应该会看到在一个epoch的训练后模型的准确性超过40%。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 3.1989\n",
      "Checking accuracy on validation set\n",
      "Got 104 / 1000 correct (10.40)\n",
      "\n",
      "Iteration 100, loss = 2.3239\n",
      "Checking accuracy on validation set\n",
      "Got 373 / 1000 correct (37.30)\n",
      "\n",
      "Iteration 200, loss = 2.1270\n",
      "Checking accuracy on validation set\n",
      "Got 373 / 1000 correct (37.30)\n",
      "\n",
      "Iteration 300, loss = 2.1504\n",
      "Checking accuracy on validation set\n",
      "Got 358 / 1000 correct (35.80)\n",
      "\n",
      "Iteration 400, loss = 1.4748\n",
      "Checking accuracy on validation set\n",
      "Got 424 / 1000 correct (42.40)\n",
      "\n",
      "Iteration 500, loss = 1.6731\n",
      "Checking accuracy on validation set\n",
      "Got 418 / 1000 correct (41.80)\n",
      "\n",
      "Iteration 600, loss = 1.6290\n",
      "Checking accuracy on validation set\n",
      "Got 442 / 1000 correct (44.20)\n",
      "\n",
      "Iteration 700, loss = 1.6193\n",
      "Checking accuracy on validation set\n",
      "Got 414 / 1000 correct (41.40)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_size = 4000\n",
    "learning_rate = 1e-2\n",
    "model = TwoLayerFC(3 * 32 * 32, hidden_layer_size, 10)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_part34(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module API: Train a Three-Layer ConvNet\n",
    "你现在应该使用模块API在CIFAR上训练三层ConvNet。这看起来应该非常类似于训练双层网络!你不需要调优任何超参数，但你应该达到45%以上的训练后一个时代。\n",
    "\n",
    "你应该使用无动量的随机梯度下降来训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "test": "module_accuracy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.3227\n",
      "Checking accuracy on validation set\n",
      "Got 108 / 1000 correct (10.80)\n",
      "\n",
      "Iteration 100, loss = 2.0137\n",
      "Checking accuracy on validation set\n",
      "Got 297 / 1000 correct (29.70)\n",
      "\n",
      "Iteration 200, loss = 1.8606\n",
      "Checking accuracy on validation set\n",
      "Got 352 / 1000 correct (35.20)\n",
      "\n",
      "Iteration 300, loss = 1.8494\n",
      "Checking accuracy on validation set\n",
      "Got 369 / 1000 correct (36.90)\n",
      "\n",
      "Iteration 400, loss = 1.8042\n",
      "Checking accuracy on validation set\n",
      "Got 416 / 1000 correct (41.60)\n",
      "\n",
      "Iteration 500, loss = 1.7413\n",
      "Checking accuracy on validation set\n",
      "Got 404 / 1000 correct (40.40)\n",
      "\n",
      "Iteration 600, loss = 1.7719\n",
      "Checking accuracy on validation set\n",
      "Got 412 / 1000 correct (41.20)\n",
      "\n",
      "Iteration 700, loss = 1.5279\n",
      "Checking accuracy on validation set\n",
      "Got 455 / 1000 correct (45.50)\n",
      "\n",
      "Iteration 0, loss = 1.7942\n",
      "Checking accuracy on validation set\n",
      "Got 468 / 1000 correct (46.80)\n",
      "\n",
      "Iteration 100, loss = 1.6910\n",
      "Checking accuracy on validation set\n",
      "Got 474 / 1000 correct (47.40)\n",
      "\n",
      "Iteration 200, loss = 1.2976\n",
      "Checking accuracy on validation set\n",
      "Got 461 / 1000 correct (46.10)\n",
      "\n",
      "Iteration 300, loss = 1.3676\n",
      "Checking accuracy on validation set\n",
      "Got 477 / 1000 correct (47.70)\n",
      "\n",
      "Iteration 400, loss = 1.6848\n",
      "Checking accuracy on validation set\n",
      "Got 484 / 1000 correct (48.40)\n",
      "\n",
      "Iteration 500, loss = 1.6594\n",
      "Checking accuracy on validation set\n",
      "Got 498 / 1000 correct (49.80)\n",
      "\n",
      "Iteration 600, loss = 1.3867\n",
      "Checking accuracy on validation set\n",
      "Got 504 / 1000 correct (50.40)\n",
      "\n",
      "Iteration 700, loss = 1.4801\n",
      "Checking accuracy on validation set\n",
      "Got 506 / 1000 correct (50.60)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-3\n",
    "channel_1 = 32\n",
    "channel_2 = 16\n",
    "\n",
    "model = None\n",
    "optimizer = None\n",
    "################################################################################\n",
    "# TODO: Instantiate your ThreeLayerConvNet model and a corresponding optimizer #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "model = ThreeLayerConvNet(3, channel_1, channel_2, 10)\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
    "train_part34(model, optimizer, epochs=1)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "\n",
    "train_part34(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV. PyTorch Sequential API\n",
    "\n",
    "第三部分介绍了PyTorch模块API，它允许您定义任意可学习层及其连通性。\n",
    "\n",
    "For simple models like a stack of feed forward layers, you still need to go through 3 steps: subclass `nn.Module`, assign layers to class attributes in `__init__`, and call each layer one by one in `forward()`. Is there a more convenient way? \n",
    "\n",
    "Fortunately, PyTorch provides a container Module called `nn.Sequential`, which merges the above steps into one. It is not as flexible as `nn.Module`, because you cannot specify more complex topology than a feed-forward stack, but it's good enough for many use cases.\n",
    "\n",
    "### Sequential API: Two-Layer Network\n",
    "Let's see how to rewrite our two-layer fully connected network example with `nn.Sequential`, and train it using the training loop defined above.\n",
    "\n",
    "Again, you don't need to tune any hyperparameters here, but you shoud achieve above 40% accuracy after one epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.3042\n",
      "Checking accuracy on validation set\n",
      "Got 145 / 1000 correct (14.50)\n",
      "\n",
      "Iteration 100, loss = 2.0466\n",
      "Checking accuracy on validation set\n",
      "Got 387 / 1000 correct (38.70)\n",
      "\n",
      "Iteration 200, loss = 1.7456\n",
      "Checking accuracy on validation set\n",
      "Got 420 / 1000 correct (42.00)\n",
      "\n",
      "Iteration 300, loss = 2.0917\n",
      "Checking accuracy on validation set\n",
      "Got 413 / 1000 correct (41.30)\n",
      "\n",
      "Iteration 400, loss = 1.8159\n",
      "Checking accuracy on validation set\n",
      "Got 438 / 1000 correct (43.80)\n",
      "\n",
      "Iteration 500, loss = 2.0000\n",
      "Checking accuracy on validation set\n",
      "Got 423 / 1000 correct (42.30)\n",
      "\n",
      "Iteration 600, loss = 2.1194\n",
      "Checking accuracy on validation set\n",
      "Got 436 / 1000 correct (43.60)\n",
      "\n",
      "Iteration 700, loss = 1.8402\n",
      "Checking accuracy on validation set\n",
      "Got 450 / 1000 correct (45.00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We need to wrap `flatten` function in a module in order to stack it\n",
    "# in nn.Sequential\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)\n",
    "\n",
    "hidden_layer_size = 4000\n",
    "learning_rate = 1e-2\n",
    "\n",
    "model = nn.Sequential(\n",
    "    Flatten(),\n",
    "    nn.Linear(3 * 32 * 32, hidden_layer_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_layer_size, 10),\n",
    ")\n",
    "\n",
    "# you can use Nesterov momentum in optim.SGD\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                     momentum=0.9, nesterov=True)\n",
    "\n",
    "train_part34(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential API: Three-Layer ConvNet\n",
    "这里你应该使用`nn.Sequential`定义和训练一个三层ConvNet，其体系结构与我们在第三部分中使用的相同:\n",
    "\n",
    "1. Convolutional layer (with bias) with 32 5x5 filters, with zero-padding of 2\n",
    "2. ReLU\n",
    "3. Convolutional layer (with bias) with 16 3x3 filters, with zero-padding of 1\n",
    "4. ReLU\n",
    "5. Fully-connected layer (with bias) to compute scores for 10 classes\n",
    "\n",
    "你可以使用默认的PyTorch权重初始化。\n",
    "\n",
    "你应该使用带有Nesterov动量0.9的随机梯度下降来优化你的模型。\n",
    "\n",
    "同样，您不需要调优任何超参数，但在一个训练周期后，您应该看到准确率超过55%。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "test": "sequential_accuracy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.3329\n",
      "Checking accuracy on validation set\n",
      "Got 87 / 1000 correct (8.70)\n",
      "\n",
      "Iteration 100, loss = 1.8282\n",
      "Checking accuracy on validation set\n",
      "Got 426 / 1000 correct (42.60)\n",
      "\n",
      "Iteration 200, loss = 1.4421\n",
      "Checking accuracy on validation set\n",
      "Got 478 / 1000 correct (47.80)\n",
      "\n",
      "Iteration 300, loss = 1.2983\n",
      "Checking accuracy on validation set\n",
      "Got 496 / 1000 correct (49.60)\n",
      "\n",
      "Iteration 400, loss = 1.3239\n",
      "Checking accuracy on validation set\n",
      "Got 531 / 1000 correct (53.10)\n",
      "\n",
      "Iteration 500, loss = 1.4000\n",
      "Checking accuracy on validation set\n",
      "Got 538 / 1000 correct (53.80)\n",
      "\n",
      "Iteration 600, loss = 1.0529\n",
      "Checking accuracy on validation set\n",
      "Got 583 / 1000 correct (58.30)\n",
      "\n",
      "Iteration 700, loss = 0.9892\n",
      "Checking accuracy on validation set\n",
      "Got 588 / 1000 correct (58.80)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "channel_1 = 32\n",
    "channel_2 = 16\n",
    "learning_rate = 1e-2\n",
    "\n",
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: Rewrite the 2-layer ConvNet with bias from Part III with the           #\n",
    "# Sequential API.                                                              #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, channel_1, 5, 1, 2),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(channel_1, channel_2, 3, 1, 1),\n",
    "    nn.ReLU(),\n",
    "    Flatten(),\n",
    "    nn.Linear(channel_2*32*32, 10)\n",
    ")\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate,\n",
    "                     momentum = 0.9, nesterov = True)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "\n",
    "train_part34(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V. CIFAR-10 开放性挑战\n",
    "\n",
    "在本节中，您可以在CIFAR-10上试验你喜欢的任何ConvNet体系结构。\n",
    "\n",
    "利用所学知识，尽可能提高准确率，越高的准确率得分越高。\n",
    "\n",
    "最低要求：对结构、超参数、损失函数和优化器进行实验，以训练一个在10个epochs内在CIFAR-10 **验证集上实现至少70%**精度的模型。可以使用上面的check_accuracy和train函数。你可以用任意一个`nn.Module` or `nn.Sequential`API。\n",
    "\n",
    "在notebook的最后描述一下你做了什么。\n",
    "\n",
    "以下是每个组件的官方API文档。注意:我们在类中称为“spatial batch norm”的东西在PyTorch中称为“BatchNorm2D”。\n",
    "\n",
    "* Layers in torch.nn package: http://pytorch.org/docs/stable/nn.html\n",
    "* Activations: http://pytorch.org/docs/stable/nn.html#non-linear-activations\n",
    "* Loss functions: http://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "* Optimizers: http://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "\n",
    "### 你可以尝试的事情:\n",
    "- **Filter size**: 上面我们使用的是5x5;更小的滤波器会更有效吗?\n",
    "- **Number of filters**: 上面我们使用了32个滤波器。是多一点好还是少一点好?\n",
    "- **Pooling vs Strided Convolution**: Do you use max pooling or just stride convolutions?\n",
    "- **Batch normalization**: 尝试在卷积层之后添加spatial batch normalization，在affine layers之后添加vanilla batch normalization。你的网络训练速度更快吗?\n",
    "- **Network architecture**: 上面的网络有两层可训练参数。有了深度网络，你能做得更好吗?可以尝试的良好架构包括:\n",
    "    - [conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [conv-relu-conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [batchnorm-relu-conv]xN -> [affine]xM -> [softmax or SVM]\n",
    "- **Global Average Pooling**: Instead of flattening and then having multiple affine layers, perform convolutions until your image gets small (7x7 or so) and then perform an average pooling operation to get to a 1x1 image picture (1, 1 , Filter#), which is then reshaped into a (Filter#) vector. This is used in [Google's Inception Network](https://arxiv.org/abs/1512.00567) (See Table 1 for their architecture).\n",
    "- **Regularization**: 添加l2权重正则化，或者使用Dropout。\n",
    "\n",
    "### Tips for training\n",
    "对于你尝试的每种网络体系结构，你都应该调优学习率和其他超参数。在这样做的时候，有几件重要的事情要记住:\n",
    "\n",
    "- 如果参数运行良好，您应该在几百次迭代中看到改进\n",
    "- 记住超参数调优的从粗到细的方法:首先测试大范围的超参数，只进行一些训练迭代，以找到有效的参数组合。\n",
    "- 一旦找到了一些似乎有效的参数集，就可以围绕这些参数进行更细致的搜索。你可能需要训练更多的时间。\n",
    "- 你应该使用验证集进行超参数搜索，并保存你的测试集，以便根据验证集选择的最佳参数对体系结构进行评估。\n",
    "\n",
    "### Going above and beyond\n",
    "还有很多其他功能可以实现，以尝试和提高你的性能。\n",
    "\n",
    "- 可选的优化器:您可以尝试Adam、Adagrad、RMSprop等。\n",
    "- 可选激活函数，如leaky ReLU, parametric ReLU, ELU, or MaxOut。\n",
    "- Model ensembles\n",
    "- 数据增强\n",
    "- New Architectures\n",
    "  - [ResNets](https://arxiv.org/abs/1512.03385) where the input from the previous layer is added to the output.\n",
    "  - [DenseNets](https://arxiv.org/abs/1608.06993) where inputs into previous layers are concatenated together.\n",
    "  - [This blog has an in-depth overview](https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32)\n",
    "\n",
    "### Have fun and happy training! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "test": "open_ended_accuracy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.2917\n",
      "Checking accuracy on validation set\n",
      "Got 105 / 1000 correct (10.50)\n",
      "\n",
      "Iteration 100, loss = 1.5540\n",
      "Checking accuracy on validation set\n",
      "Got 366 / 1000 correct (36.60)\n",
      "\n",
      "Iteration 200, loss = 1.1888\n",
      "Checking accuracy on validation set\n",
      "Got 466 / 1000 correct (46.60)\n",
      "\n",
      "Iteration 300, loss = 1.3031\n",
      "Checking accuracy on validation set\n",
      "Got 356 / 1000 correct (35.60)\n",
      "\n",
      "Iteration 400, loss = 1.1746\n",
      "Checking accuracy on validation set\n",
      "Got 563 / 1000 correct (56.30)\n",
      "\n",
      "Iteration 500, loss = 1.2037\n",
      "Checking accuracy on validation set\n",
      "Got 558 / 1000 correct (55.80)\n",
      "\n",
      "Iteration 600, loss = 0.9049\n",
      "Checking accuracy on validation set\n",
      "Got 653 / 1000 correct (65.30)\n",
      "\n",
      "Iteration 700, loss = 0.9419\n",
      "Checking accuracy on validation set\n",
      "Got 604 / 1000 correct (60.40)\n",
      "\n",
      "Iteration 0, loss = 0.9000\n",
      "Checking accuracy on validation set\n",
      "Got 561 / 1000 correct (56.10)\n",
      "\n",
      "Iteration 100, loss = 0.8508\n",
      "Checking accuracy on validation set\n",
      "Got 705 / 1000 correct (70.50)\n",
      "\n",
      "Iteration 200, loss = 0.6539\n",
      "Checking accuracy on validation set\n",
      "Got 664 / 1000 correct (66.40)\n",
      "\n",
      "Iteration 300, loss = 0.7645\n",
      "Checking accuracy on validation set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "\n",
      "Iteration 400, loss = 0.5948\n",
      "Checking accuracy on validation set\n",
      "Got 711 / 1000 correct (71.10)\n",
      "\n",
      "Iteration 500, loss = 0.8494\n",
      "Checking accuracy on validation set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "\n",
      "Iteration 600, loss = 0.8143\n",
      "Checking accuracy on validation set\n",
      "Got 706 / 1000 correct (70.60)\n",
      "\n",
      "Iteration 700, loss = 0.5965\n",
      "Checking accuracy on validation set\n",
      "Got 737 / 1000 correct (73.70)\n",
      "\n",
      "Iteration 0, loss = 0.7937\n",
      "Checking accuracy on validation set\n",
      "Got 644 / 1000 correct (64.40)\n",
      "\n",
      "Iteration 100, loss = 0.6434\n",
      "Checking accuracy on validation set\n",
      "Got 713 / 1000 correct (71.30)\n",
      "\n",
      "Iteration 200, loss = 0.5584\n",
      "Checking accuracy on validation set\n",
      "Got 721 / 1000 correct (72.10)\n",
      "\n",
      "Iteration 300, loss = 0.8461\n",
      "Checking accuracy on validation set\n",
      "Got 750 / 1000 correct (75.00)\n",
      "\n",
      "Iteration 400, loss = 0.6983\n",
      "Checking accuracy on validation set\n",
      "Got 740 / 1000 correct (74.00)\n",
      "\n",
      "Iteration 500, loss = 0.5902\n",
      "Checking accuracy on validation set\n",
      "Got 725 / 1000 correct (72.50)\n",
      "\n",
      "Iteration 600, loss = 0.9074\n",
      "Checking accuracy on validation set\n",
      "Got 762 / 1000 correct (76.20)\n",
      "\n",
      "Iteration 700, loss = 0.5244\n",
      "Checking accuracy on validation set\n",
      "Got 777 / 1000 correct (77.70)\n",
      "\n",
      "Iteration 0, loss = 0.4145\n",
      "Checking accuracy on validation set\n",
      "Got 774 / 1000 correct (77.40)\n",
      "\n",
      "Iteration 100, loss = 0.3829\n",
      "Checking accuracy on validation set\n",
      "Got 788 / 1000 correct (78.80)\n",
      "\n",
      "Iteration 200, loss = 0.4046\n",
      "Checking accuracy on validation set\n",
      "Got 779 / 1000 correct (77.90)\n",
      "\n",
      "Iteration 300, loss = 0.3979\n",
      "Checking accuracy on validation set\n",
      "Got 775 / 1000 correct (77.50)\n",
      "\n",
      "Iteration 400, loss = 0.7841\n",
      "Checking accuracy on validation set\n",
      "Got 759 / 1000 correct (75.90)\n",
      "\n",
      "Iteration 500, loss = 0.3026\n",
      "Checking accuracy on validation set\n",
      "Got 797 / 1000 correct (79.70)\n",
      "\n",
      "Iteration 600, loss = 0.2757\n",
      "Checking accuracy on validation set\n",
      "Got 810 / 1000 correct (81.00)\n",
      "\n",
      "Iteration 700, loss = 0.5067\n",
      "Checking accuracy on validation set\n",
      "Got 794 / 1000 correct (79.40)\n",
      "\n",
      "Iteration 0, loss = 0.4542\n",
      "Checking accuracy on validation set\n",
      "Got 806 / 1000 correct (80.60)\n",
      "\n",
      "Iteration 100, loss = 0.4629\n",
      "Checking accuracy on validation set\n",
      "Got 790 / 1000 correct (79.00)\n",
      "\n",
      "Iteration 200, loss = 0.3583\n",
      "Checking accuracy on validation set\n",
      "Got 789 / 1000 correct (78.90)\n",
      "\n",
      "Iteration 300, loss = 0.4277\n",
      "Checking accuracy on validation set\n",
      "Got 793 / 1000 correct (79.30)\n",
      "\n",
      "Iteration 400, loss = 0.4010\n",
      "Checking accuracy on validation set\n",
      "Got 787 / 1000 correct (78.70)\n",
      "\n",
      "Iteration 500, loss = 0.2802\n",
      "Checking accuracy on validation set\n",
      "Got 809 / 1000 correct (80.90)\n",
      "\n",
      "Iteration 600, loss = 0.4337\n",
      "Checking accuracy on validation set\n",
      "Got 800 / 1000 correct (80.00)\n",
      "\n",
      "Iteration 700, loss = 0.4437\n",
      "Checking accuracy on validation set\n",
      "Got 809 / 1000 correct (80.90)\n",
      "\n",
      "Iteration 0, loss = 0.2920\n",
      "Checking accuracy on validation set\n",
      "Got 813 / 1000 correct (81.30)\n",
      "\n",
      "Iteration 100, loss = 0.3117\n",
      "Checking accuracy on validation set\n",
      "Got 813 / 1000 correct (81.30)\n",
      "\n",
      "Iteration 200, loss = 0.2323\n",
      "Checking accuracy on validation set\n",
      "Got 815 / 1000 correct (81.50)\n",
      "\n",
      "Iteration 300, loss = 0.4903\n",
      "Checking accuracy on validation set\n",
      "Got 825 / 1000 correct (82.50)\n",
      "\n",
      "Iteration 400, loss = 0.3615\n",
      "Checking accuracy on validation set\n",
      "Got 808 / 1000 correct (80.80)\n",
      "\n",
      "Iteration 500, loss = 0.5228\n",
      "Checking accuracy on validation set\n",
      "Got 809 / 1000 correct (80.90)\n",
      "\n",
      "Iteration 600, loss = 0.3314\n",
      "Checking accuracy on validation set\n",
      "Got 790 / 1000 correct (79.00)\n",
      "\n",
      "Iteration 700, loss = 0.2211\n",
      "Checking accuracy on validation set\n",
      "Got 819 / 1000 correct (81.90)\n",
      "\n",
      "Iteration 0, loss = 0.3180\n",
      "Checking accuracy on validation set\n",
      "Got 803 / 1000 correct (80.30)\n",
      "\n",
      "Iteration 100, loss = 0.2034\n",
      "Checking accuracy on validation set\n",
      "Got 819 / 1000 correct (81.90)\n",
      "\n",
      "Iteration 200, loss = 0.3583\n",
      "Checking accuracy on validation set\n",
      "Got 815 / 1000 correct (81.50)\n",
      "\n",
      "Iteration 300, loss = 0.3126\n",
      "Checking accuracy on validation set\n",
      "Got 838 / 1000 correct (83.80)\n",
      "\n",
      "Iteration 400, loss = 0.2938\n",
      "Checking accuracy on validation set\n",
      "Got 827 / 1000 correct (82.70)\n",
      "\n",
      "Iteration 500, loss = 0.2327\n",
      "Checking accuracy on validation set\n",
      "Got 832 / 1000 correct (83.20)\n",
      "\n",
      "Iteration 600, loss = 0.2483\n",
      "Checking accuracy on validation set\n",
      "Got 796 / 1000 correct (79.60)\n",
      "\n",
      "Iteration 700, loss = 0.3271\n",
      "Checking accuracy on validation set\n",
      "Got 814 / 1000 correct (81.40)\n",
      "\n",
      "Iteration 0, loss = 0.2247\n",
      "Checking accuracy on validation set\n",
      "Got 823 / 1000 correct (82.30)\n",
      "\n",
      "Iteration 100, loss = 0.2572\n",
      "Checking accuracy on validation set\n",
      "Got 811 / 1000 correct (81.10)\n",
      "\n",
      "Iteration 200, loss = 0.1269\n",
      "Checking accuracy on validation set\n",
      "Got 832 / 1000 correct (83.20)\n",
      "\n",
      "Iteration 300, loss = 0.2600\n",
      "Checking accuracy on validation set\n",
      "Got 815 / 1000 correct (81.50)\n",
      "\n",
      "Iteration 400, loss = 0.1738\n",
      "Checking accuracy on validation set\n",
      "Got 814 / 1000 correct (81.40)\n",
      "\n",
      "Iteration 500, loss = 0.2120\n",
      "Checking accuracy on validation set\n",
      "Got 817 / 1000 correct (81.70)\n",
      "\n",
      "Iteration 600, loss = 0.2484\n",
      "Checking accuracy on validation set\n",
      "Got 834 / 1000 correct (83.40)\n",
      "\n",
      "Iteration 700, loss = 0.2826\n",
      "Checking accuracy on validation set\n",
      "Got 841 / 1000 correct (84.10)\n",
      "\n",
      "Iteration 0, loss = 0.1514\n",
      "Checking accuracy on validation set\n",
      "Got 842 / 1000 correct (84.20)\n",
      "\n",
      "Iteration 100, loss = 0.1523\n",
      "Checking accuracy on validation set\n",
      "Got 841 / 1000 correct (84.10)\n",
      "\n",
      "Iteration 200, loss = 0.1996\n",
      "Checking accuracy on validation set\n",
      "Got 832 / 1000 correct (83.20)\n",
      "\n",
      "Iteration 300, loss = 0.2029\n",
      "Checking accuracy on validation set\n",
      "Got 816 / 1000 correct (81.60)\n",
      "\n",
      "Iteration 400, loss = 0.2010\n",
      "Checking accuracy on validation set\n",
      "Got 835 / 1000 correct (83.50)\n",
      "\n",
      "Iteration 500, loss = 0.2042\n",
      "Checking accuracy on validation set\n",
      "Got 834 / 1000 correct (83.40)\n",
      "\n",
      "Iteration 600, loss = 0.1972\n",
      "Checking accuracy on validation set\n",
      "Got 830 / 1000 correct (83.00)\n",
      "\n",
      "Iteration 700, loss = 0.2771\n",
      "Checking accuracy on validation set\n",
      "Got 840 / 1000 correct (84.00)\n",
      "\n",
      "Iteration 0, loss = 0.1217\n",
      "Checking accuracy on validation set\n",
      "Got 842 / 1000 correct (84.20)\n",
      "\n",
      "Iteration 100, loss = 0.0951\n",
      "Checking accuracy on validation set\n",
      "Got 842 / 1000 correct (84.20)\n",
      "\n",
      "Iteration 200, loss = 0.1913\n",
      "Checking accuracy on validation set\n",
      "Got 823 / 1000 correct (82.30)\n",
      "\n",
      "Iteration 300, loss = 0.1360\n",
      "Checking accuracy on validation set\n",
      "Got 828 / 1000 correct (82.80)\n",
      "\n",
      "Iteration 400, loss = 0.2871\n",
      "Checking accuracy on validation set\n",
      "Got 818 / 1000 correct (81.80)\n",
      "\n",
      "Iteration 500, loss = 0.1463\n",
      "Checking accuracy on validation set\n",
      "Got 843 / 1000 correct (84.30)\n",
      "\n",
      "Iteration 600, loss = 0.2359\n",
      "Checking accuracy on validation set\n",
      "Got 839 / 1000 correct (83.90)\n",
      "\n",
      "Iteration 700, loss = 0.2483\n",
      "Checking accuracy on validation set\n",
      "Got 795 / 1000 correct (79.50)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# TODO:                                                                        #         \n",
    "# Experiment with any architectures, optimizers, and hyperparameters.          #\n",
    "# Achieve AT LEAST 70% accuracy on the *validation set* within 10 epochs.      #\n",
    "#                                                                              #\n",
    "# Note that you can use the check_accuracy function to evaluate on either      #\n",
    "# the test set or the validation set, by passing either loader_test or         #\n",
    "# loader_val as the second argument to check_accuracy. You should not touch    #\n",
    "# the test set until you have finished your architecture and  hyperparameter   #\n",
    "# tuning, and only run the test set once at the end to report a final value.   #\n",
    "################################################################################\n",
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "learning_rate = 1e-2\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, (3, 3), padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 32, (3, 3), padding=1),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d((2, 2)),\n",
    "    nn.Conv2d(32, 64, (3, 3), padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, (3, 3), padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d((2, 2)),\n",
    "    nn.Conv2d(64, 128, (3, 3), padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(128, 128, (3, 3), padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(128, 128, (3, 3), padding=1),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d((2, 2)),\n",
    "    Flatten(),\n",
    "    nn.Linear(128 * 4 * 4, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10),\n",
    ")\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                      momentum=0.9, nesterov=True)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "\n",
    "# You should get at least 70% accuracy\n",
    "train_part34(model, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "## 描述你做了什么\n",
    "\n",
    "在下面的单元格中，你应该解释你做了什么，你实现的任何附加功能，和/或你在训练和评估网络过程中制作的任何图表。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "网络架构：  \n",
    "卷积层，激活函数，卷积层，批次归一化，激活函数，最大池化层(卷积核大小：3 * 3，激活函数RELU)  \n",
    "卷积层，激活函数，卷积层，批次归一化，激活函数，最大池化层(卷积核大小：3 * 3，激活函数RELU)  \n",
    "卷积层，激活函数，卷积层，激活函数，卷积层，批次归一化，激活函数，最大池化层(卷积核大小：3 * 3，激活函数RELU)  \n",
    "全连接层，激活函数，全连接层，激活函数，全连接层(激活函数RELU)  \n",
    "优化：SGD随机梯度下降\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set -- run this only once\n",
    "\n",
    "现在我们已经得到了满意的结果，我们在测试集中测试最终的模型(应该存储在best_model中)。想一想这与您的验证集准确性如何比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\n\u001b[0;32m      2\u001b[0m check_accuracy_part34(loader_test, best_model)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "best_model = model\n",
    "check_accuracy_part34(loader_test, best_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
